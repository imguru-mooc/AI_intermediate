{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bd329a4bbca"
   },
   "source": [
    "# Masking and padding with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d4ac441b1fc"
   },
   "source": [
    "## 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T02:09:00.004888Z",
     "iopub.status.busy": "2021-01-15T02:09:00.004278Z",
     "iopub.status.idle": "2021-01-15T02:09:06.339297Z",
     "shell.execute_reply": "2021-01-15T02:09:06.339724Z"
    },
    "id": "ec52be14e686"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3731f3ff3670"
   },
   "source": [
    "## 시작하기\n",
    "\n",
    "**마스킹** 은 시퀀스 처리 레이어에 입력의 특정 시간 단계가 누락되어 데이터를 처리 할 때 건너 뛰도록 지시하는 방법입니다.\n",
    "\n",
    "**패딩**은 마스킹된 스텝이 시퀀스의 시작 또는 도입부에 위치하는 특수한 형태의 마스킹입니다. 패딩이 필요한 이유는 시퀀스 데이터를 연속 배치로 인코딩해야 하는 데 있습니다. 배치의 모든 시퀀스를 지정된 표준 길이에 맞추려면 일부 시퀀스를 패딩 처리하거나 잘라내야 합니다.\n",
    "\n",
    "자세히 살펴 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ac6b121d6be0"
   },
   "source": [
    "## 패딩 시퀀스 데이터\n",
    "\n",
    "시퀀스 데이터를 처리 할 때 개별 샘플의 길이가 다른 것이 매우 일반적입니다. 다음 예제를 고려하십시오 (텍스트로 단어로 토큰 화됨).\n",
    "\n",
    "```\n",
    "[\n",
    "  [\"Hello\", \"world\", \"!\"],\n",
    "  [\"How\", \"are\", \"you\", \"doing\", \"today\"],\n",
    "  [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"],\n",
    "]\n",
    "```\n",
    "\n",
    "어휘 조회 후 데이터는 다음과 같이 정수로 벡터화 될 수 있습니다.\n",
    "\n",
    "```\n",
    "[\n",
    "  [71, 1331, 4231]\n",
    "  [73, 8, 3215, 55, 927],\n",
    "  [83, 91, 1, 645, 1253, 927],\n",
    "]\n",
    "```\n",
    "\n",
    "데이터는 개별 샘플의 길이가 각각 3, 5 및 6인 중첩된 목록입니다. 딥 러닝 모델의 입력 데이터는 단일 텐서(이 경우, 예를 들어 `(batch_size, 6, vocab_size)` 형상의 텐서)여야 하므로 가장 긴 항목보다 짧은 샘플은 일부 자리 표시자 값으로 패딩 처리해야 합니다(또는, 짧은 샘플을 패딩 처리하기 전에 긴 샘플을 잘라낼 수도 있음).\n",
    "\n",
    "Keras는 Python 목록을 잘라서 공통 길이로 패딩 처리하는 유틸리티 기능을 제공합니다: `tf.keras.preprocessing.sequence.pad_sequences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T02:09:06.345936Z",
     "iopub.status.busy": "2021-01-15T02:09:06.345282Z",
     "iopub.status.idle": "2021-01-15T02:09:06.349400Z",
     "shell.execute_reply": "2021-01-15T02:09:06.349819Z"
    },
    "id": "bb64fb185a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 711  632   71    0    0    0]\n",
      " [  73    8 3215   55  927    0]\n",
      " [  83   91    1  645 1253  927]]\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    [711, 632, 71],\n",
    "    [73, 8, 3215, 55, 927],\n",
    "    [83, 91, 1, 645, 1253, 927],\n",
    "]\n",
    "\n",
    "# By default, this will pad using 0s; it is configurable via the\n",
    "# \"value\" parameter.\n",
    "# Note that you could \"pre\" padding (at the beginning) or\n",
    "# \"post\" padding (at the end).\n",
    "# We recommend using \"post\" padding when working with RNN layers\n",
    "# (in order to be able to use the\n",
    "# CuDNN implementation of the layers).\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    raw_inputs, padding=\"post\"\n",
    ")\n",
    "print(padded_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03092b2da690"
   },
   "source": [
    "## 마스킹\n",
    "\n",
    "이제 모든 샘플의 길이가 균일하므로 데이터의 일부가 실제로 채워져 있다는 사실을 모델에 알려야합니다. 그 메커니즘은 **마스킹** 입니다.\n",
    "\n",
    "Keras 모델에서 입력 마스크를 도입하는 세 가지 방법이 있습니다.\n",
    "\n",
    "- `keras.layers.Masking` 레이어를 추가하십시오.\n",
    "- `mask_zero=True` 로 `keras.layers.Embedding` 레이어를 구성하십시오.\n",
    "- 이 인수를 지원하는 계층 (예 : RNN 계층)을 호출 할 때 `mask` 인수를 수동으로 전달하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6103601e5fff"
   },
   "source": [
    "## 마스크 생성 레이어 : `Embedding` 및 `Masking`\n",
    "\n",
    "후드 아래에서이 레이어는 마스크 텐서 (모양 `(batch, sequence_length)` 가진 2D 텐서)를 만들어 `Masking` 또는 `Embedding` 레이어에서 반환 한 텐서 출력에 연결합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n",
      "(3, 6, 1)\n",
      "(3, 6, 10)\n",
      "tf.Tensor(\n",
      "[[[ 711  711  711  711  711  711  711  711  711  711]\n",
      "  [ 632  632  632  632  632  632  632  632  632  632]\n",
      "  [  71   71   71   71   71   71   71   71   71   71]\n",
      "  [   0    0    0    0    0    0    0    0    0    0]\n",
      "  [   0    0    0    0    0    0    0    0    0    0]\n",
      "  [   0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      " [[  73   73   73   73   73   73   73   73   73   73]\n",
      "  [   8    8    8    8    8    8    8    8    8    8]\n",
      "  [3215 3215 3215 3215 3215 3215 3215 3215 3215 3215]\n",
      "  [  55   55   55   55   55   55   55   55   55   55]\n",
      "  [ 927  927  927  927  927  927  927  927  927  927]\n",
      "  [   0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      " [[  83   83   83   83   83   83   83   83   83   83]\n",
      "  [  91   91   91   91   91   91   91   91   91   91]\n",
      "  [   1    1    1    1    1    1    1    1    1    1]\n",
      "  [ 645  645  645  645  645  645  645  645  645  645]\n",
      "  [1253 1253 1253 1253 1253 1253 1253 1253 1253 1253]\n",
      "  [ 927  927  927  927  927  927  927  927  927  927]]], shape=(3, 6, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(np.array([[ 711,  632,   71,    0,    0,    0],\n",
    "                          [  73,    8, 3215,   55,  927,    0],\n",
    "                          [  83,   91,    1,  645, 1253,  927]]))\n",
    "print(a.shape)\n",
    "a = tf.expand_dims(a, axis=-1)\n",
    "print(a.shape)\n",
    "a = tf.tile(a, [1, 1, 10])\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T02:09:06.355653Z",
     "iopub.status.busy": "2021-01-15T02:09:06.355012Z",
     "iopub.status.idle": "2021-01-15T02:09:08.145698Z",
     "shell.execute_reply": "2021-01-15T02:09:08.146105Z"
    },
    "id": "b2363b293483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OVERLOADABLE_OPERATORS', '_USE_EQUALITY', '__abs__', '__add__', '__and__', '__array__', '__array_priority__', '__bool__', '__class__', '__complex__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__iter__', '__le__', '__len__', '__long__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '__xor__', '_add_consumer', '_as_node_def_input', '_as_tf_output', '_c_api_shape', '_copy', '_copy_nograd', '_copy_to_device', '_create_with_tf_output', '_datatype_enum', '_disallow_bool_casting', '_disallow_in_graph_mode', '_disallow_iteration', '_disallow_when_autograph_disabled', '_disallow_when_autograph_enabled', '_handle_data', '_id', '_keras_mask', '_num_elements', '_numpy', '_numpy_internal', '_override_operator', '_rank', '_shape', '_shape_as_list', '_shape_tuple', '_tensor_shape', '_tf_api_names', '_tf_api_names_v1', 'backing_device', 'consumers', 'cpu', 'device', 'dtype', 'eval', 'experimental_ref', 'get_shape', 'gpu', 'graph', 'is_packed', 'name', 'ndim', 'numpy', 'op', 'ref', 'set_shape', 'shape', 'value_index']\n",
      "<tf.Tensor: shape=(3, 6, 16), dtype=float32, numpy=\n",
      "array([[[ 0.03137771,  0.04855636,  0.01194427, -0.01858104,\n",
      "          0.0100523 , -0.02384748, -0.00692765, -0.02170845,\n",
      "          0.01633104,  0.03300745,  0.01893422, -0.00549463,\n",
      "          0.04359684,  0.03337039,  0.03419318,  0.01153871],\n",
      "        [ 0.0464821 , -0.04371225,  0.04354116, -0.01049332,\n",
      "         -0.03843759,  0.00677154, -0.03011472,  0.00670404,\n",
      "         -0.01360611,  0.02172295,  0.02592457, -0.01250043,\n",
      "          0.00348015, -0.01298903,  0.04655726,  0.02336017],\n",
      "        [-0.02103254, -0.0362211 ,  0.0134995 , -0.02902938,\n",
      "          0.00638379, -0.01150388, -0.0035455 , -0.04123844,\n",
      "          0.00649088, -0.00300925, -0.03474857,  0.03532184,\n",
      "          0.0284296 ,  0.03691306,  0.04889112, -0.03144202],\n",
      "        [ 0.00757351, -0.02383386,  0.00578858, -0.00209007,\n",
      "          0.04466672, -0.04188099, -0.0397196 ,  0.00368885,\n",
      "         -0.00212263, -0.01447322,  0.04052225, -0.00352175,\n",
      "         -0.03932074,  0.02654526,  0.03639051,  0.01290211],\n",
      "        [ 0.00757351, -0.02383386,  0.00578858, -0.00209007,\n",
      "          0.04466672, -0.04188099, -0.0397196 ,  0.00368885,\n",
      "         -0.00212263, -0.01447322,  0.04052225, -0.00352175,\n",
      "         -0.03932074,  0.02654526,  0.03639051,  0.01290211],\n",
      "        [ 0.00757351, -0.02383386,  0.00578858, -0.00209007,\n",
      "          0.04466672, -0.04188099, -0.0397196 ,  0.00368885,\n",
      "         -0.00212263, -0.01447322,  0.04052225, -0.00352175,\n",
      "         -0.03932074,  0.02654526,  0.03639051,  0.01290211]],\n",
      "\n",
      "       [[-0.00624371,  0.02028691, -0.04939131, -0.04947526,\n",
      "         -0.01771947,  0.0293032 ,  0.03166829, -0.02263769,\n",
      "          0.03500085, -0.01066957, -0.02263401,  0.01099137,\n",
      "          0.02651722, -0.01160017, -0.03484767,  0.0384762 ],\n",
      "        [ 0.00588218, -0.04124258, -0.01606511,  0.00541181,\n",
      "          0.04497181,  0.02713767, -0.03376807, -0.00442473,\n",
      "         -0.03969936, -0.012693  , -0.02102897, -0.04461836,\n",
      "          0.010765  ,  0.03676296, -0.0256828 ,  0.00849881],\n",
      "        [ 0.03435046, -0.04847009,  0.0042547 , -0.00417782,\n",
      "         -0.00461159, -0.01477572,  0.04603329, -0.01100068,\n",
      "          0.02953167,  0.0444193 , -0.02993393, -0.0473615 ,\n",
      "          0.02512393, -0.04464434,  0.02169032,  0.03692672],\n",
      "        [ 0.04777214, -0.04029552, -0.04436439, -0.01478289,\n",
      "          0.02032442, -0.0216073 ,  0.01823199,  0.01269947,\n",
      "          0.01427862, -0.04580212,  0.02802458, -0.01261343,\n",
      "          0.00886275, -0.02735858, -0.01173856, -0.04880822],\n",
      "        [-0.00598892,  0.03799846,  0.03147814,  0.00385235,\n",
      "          0.03346194, -0.00670797,  0.01512818,  0.00722684,\n",
      "         -0.01653268, -0.01913095,  0.03239686, -0.0058589 ,\n",
      "         -0.0295163 , -0.04411576, -0.00230743, -0.0326697 ],\n",
      "        [ 0.00757351, -0.02383386,  0.00578858, -0.00209007,\n",
      "          0.04466672, -0.04188099, -0.0397196 ,  0.00368885,\n",
      "         -0.00212263, -0.01447322,  0.04052225, -0.00352175,\n",
      "         -0.03932074,  0.02654526,  0.03639051,  0.01290211]],\n",
      "\n",
      "       [[ 0.03269452, -0.02532599,  0.03844203,  0.02393444,\n",
      "          0.04899809, -0.02943711, -0.02156043,  0.0209331 ,\n",
      "          0.03284417,  0.0414753 , -0.02399583,  0.02392675,\n",
      "         -0.03390938, -0.04864059, -0.0316707 , -0.0209602 ],\n",
      "        [-0.03286403,  0.02315888, -0.04488435,  0.01771677,\n",
      "          0.00793689, -0.01901081,  0.02189666, -0.04950235,\n",
      "          0.01189679, -0.02571015, -0.01208614, -0.0096452 ,\n",
      "          0.01851692,  0.01762344, -0.0390022 , -0.01606255],\n",
      "        [ 0.02024055, -0.01356066,  0.0371695 ,  0.008265  ,\n",
      "          0.02789837,  0.00063806, -0.03541261,  0.0432876 ,\n",
      "          0.01019301, -0.0214898 , -0.0417117 ,  0.03866107,\n",
      "         -0.00693838, -0.0201507 ,  0.03856883, -0.02851654],\n",
      "        [ 0.01187402, -0.04175264, -0.01096372, -0.03514498,\n",
      "         -0.03897329,  0.00366767, -0.04120551, -0.0456895 ,\n",
      "         -0.02117887,  0.01398597, -0.00373288,  0.00886216,\n",
      "         -0.03257953,  0.0438402 ,  0.01949158, -0.02902362],\n",
      "        [-0.04597205, -0.0090251 ,  0.00223534, -0.03019017,\n",
      "          0.04842781, -0.03012948,  0.03527753,  0.01390531,\n",
      "          0.02596169,  0.01002312, -0.00744535, -0.01310053,\n",
      "          0.00544949, -0.02631431, -0.04126562,  0.03275788],\n",
      "        [-0.00598892,  0.03799846,  0.03147814,  0.00385235,\n",
      "          0.03346194, -0.00670797,  0.01512818,  0.00722684,\n",
      "         -0.01653268, -0.01913095,  0.03239686, -0.0058589 ,\n",
      "         -0.0295163 , -0.04411576, -0.00230743, -0.0326697 ]]],\n",
      "      dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[ True  True  True False False False]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "masked_output = embedding(padded_inputs)\n",
    "# print(masked_output)\n",
    "print(dir(masked_output))\n",
    "print(repr(masked_output))\n",
    "print(masked_output._keras_mask)\n",
    "\n",
    "# masking_layer = layers.Masking()\n",
    "# Simulate the embedding lookup by expanding the 2D input to 3D,\n",
    "# with embedding dimension of 10.\n",
    "# unmasked_embedding = tf.cast(\n",
    "#     tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1, 1, 10]), tf.float32\n",
    "# )\n",
    "# print(unmasked_embedding.shape)\n",
    "# print(unmasked_embedding)\n",
    "\n",
    "# masked_embedding = masking_layer(unmasked_embedding)\n",
    "# print(masked_embedding._keras_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17e4bdb563b2"
   },
   "source": [
    "인쇄 된 결과에서 볼 수 있듯이 마스크는 모양이 `(batch_size, sequence_length)` 인 2D 부울 텐서이며, 각 개별 `False` 항목은 처리 중에 해당 시간 단계를 무시해야 함을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf11a0399fcf"
   },
   "source": [
    "## Functional API 및 Sequential API의 마스크 전파\n",
    "\n",
    "Functional API 또는 Sequential API를 사용하는 경우 `Embedding` 또는 `Masking` 계층에서 생성 된 마스크는이를 사용할 수있는 계층 (예 : RNN 계층)에 대해 네트워크를 통해 전파됩니다. Keras는 입력에 해당하는 마스크를 자동으로 가져 와서 사용 방법을 알고있는 레이어로 전달합니다.\n",
    "\n",
    "예를 들어, 다음 순차 모델에서 `LSTM` 레이어는 마스크를 자동으로 수신하므로 패딩 처리된 값을 무시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T02:09:08.159542Z",
     "iopub.status.busy": "2021-01-15T02:09:08.154283Z",
     "iopub.status.idle": "2021-01-15T02:09:08.912278Z",
     "shell.execute_reply": "2021-01-15T02:09:08.911643Z"
    },
    "id": "0adbecda288a"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True), layers.LSTM(32),]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8ac6481a1d5"
   },
   "source": [
    "다음과 같은 기능적 API 모델의 경우에도 마찬가지입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T02:09:08.919917Z",
     "iopub.status.busy": "2021-01-15T02:09:08.919077Z",
     "iopub.status.idle": "2021-01-15T02:09:09.631945Z",
     "shell.execute_reply": "2021-01-15T02:09:09.631253Z"
    },
    "id": "f374ab06743d"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
    "outputs = layers.LSTM(32)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2f2c4b96ecb5"
   },
   "source": [
    "## 마스크 텐서를 레이어로 직접 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11dccb581014"
   },
   "source": [
    "마스크를 처리 할 수있는 레이어 (예 : `LSTM` 레이어)는 `__call__` 메서드에 `mask` 인수가 있습니다.\n",
    "\n",
    "한편 마스크 (예 : `Embedding` )를 생성하는 레이어는 호출 할 수있는 `compute_mask(input, previous_mask)` 메소드를 노출합니다.\n",
    "\n",
    "따라서 마스크 생성 레이어의 `compute_mask()` 메서드 출력을 다음과 같이 마스크 소비 레이어의 `__call__` 메서드로 전달할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T02:09:09.649197Z",
     "iopub.status.busy": "2021-01-15T02:09:09.648409Z",
     "iopub.status.idle": "2021-01-15T02:09:10.104451Z",
     "shell.execute_reply": "2021-01-15T02:09:10.104923Z"
    },
    "id": "1955aa63896b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True False  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True False  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True False  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True False  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]], shape=(32, 10), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True False  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True False  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True False  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True False  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]], shape=(32, 10), dtype=bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[-0.00574443,  0.00083171,  0.00104074, ..., -0.00378538,\n",
       "        -0.00421865, -0.00044938],\n",
       "       [-0.0025338 , -0.00475884, -0.00240111, ..., -0.00226971,\n",
       "         0.00097289,  0.0026074 ],\n",
       "       [-0.00382672, -0.00642035,  0.00408791, ...,  0.00488193,\n",
       "         0.00261205, -0.00290455],\n",
       "       ...,\n",
       "       [-0.00273553,  0.00747044, -0.01332129, ..., -0.00189304,\n",
       "        -0.00889491, -0.00468274],\n",
       "       [-0.00086024, -0.00504664, -0.0010674 , ...,  0.00134363,\n",
       "         0.00363894, -0.00026289],\n",
       "       [-0.00058815, -0.00454287, -0.00846209, ..., -0.0067899 ,\n",
       "         0.009823  ,  0.00604398]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "        self.embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "        self.lstm = layers.LSTM(32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        # Note that you could also prepare a `mask` tensor manually.\n",
    "        # It only needs to be a boolean tensor\n",
    "        # with the right shape, i.e. (batch_size, timesteps).\n",
    "        print(x._keras_mask)\n",
    "        mask = self.embedding.compute_mask(inputs)\n",
    "        print(mask)\n",
    "        output = self.lstm(x, mask=mask)  # The layer will ignore the masked values\n",
    "        return output\n",
    "\n",
    "\n",
    "layer = MyLayer()\n",
    "x = np.random.random((32, 10)) * 100\n",
    "x = x.astype(\"int32\")\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b04dd330f848"
   },
   "source": [
    "## 사용자 정의 레이어에서 마스킹 지원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8451a1a8ff27"
   },
   "source": [
    "때로는 마스크를 생성하는 레이어(예: `Embedding`) 또는 현재 마스크를 수정해야 하는 레이어를 작성해야 할 수도 있습니다.\n",
    "\n",
    "예를 들어, 시간 차원에서 연결된 `Concatenate` 레이어와 같이 입력과 다른 시간 차원을 가진 텐서를 생성하는 레이어는 다운스트림 레이어가 마스킹된 타임스텝을 올바르게 처리할 수 있도록 현재 마스크를 수정해야 합니다.\n",
    "\n",
    "To do this, your layer should implement the `layer.compute_mask()` method, which produces a new mask given the input and the current mask.\n",
    "\n",
    "Here is an example of a `TemporalSplit` layer that needs to modify the current mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [3]], shape=(2, 1), dtype=int32) tf.Tensor(\n",
      "[[2]\n",
      " [4]], shape=(2, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(np.array([[1,2],\n",
    "                          [3,4]]))\n",
    "b = tf.split(a, 2, axis=1)\n",
    "print(b[0], b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T02:09:10.112462Z",
     "iopub.status.busy": "2021-01-15T02:09:10.111755Z",
     "iopub.status.idle": "2021-01-15T02:09:10.115023Z",
     "shell.execute_reply": "2021-01-15T02:09:10.115439Z"
    },
    "id": "a06fb2194c0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask tf.Tensor(\n",
      "[[ True  True  True False False False]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "class TemporalSplit(keras.layers.Layer):\n",
    "    \"\"\"Split the input tensor into 2 tensors along the time dimension.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Expect the input to be 3D and mask to be 2D, split the input tensor into 2\n",
    "        # subtensors along the time axis (axis 1).\n",
    "#         print(tf.split(inputs, 2, axis=1))\n",
    "        return tf.split(inputs, 2, axis=1)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Also split the mask into 2 if it presents.\n",
    "        if mask is None:\n",
    "            return None\n",
    "        print('mask', mask)\n",
    "        return tf.split(mask, 2, axis=1)\n",
    "\n",
    "\n",
    "# print(masked_embedding._keras_mask)\n",
    "first_half, second_half = TemporalSplit()(masked_embedding)\n",
    "# print(first_half._keras_mask)\n",
    "# print(second_half._keras_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "282b867dcd95"
   },
   "source": [
    "입력 값에서 마스크를 생성 할 수있는 `CustomEmbedding` 레이어의 다른 예는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T02:09:10.124940Z",
     "iopub.status.busy": "2021-01-15T02:09:10.124233Z",
     "iopub.status.idle": "2021-01-15T02:09:10.128925Z",
     "shell.execute_reply": "2021-01-15T02:09:10.128423Z"
    },
    "id": "e760655cd39c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[False  True  True False  True False  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True]], shape=(3, 10), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "class CustomEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):\n",
    "        super(CustomEmbedding, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.mask_zero = mask_zero\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embeddings = self.add_weight(\n",
    "            shape=(self.input_dim, self.output_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.embeddings, inputs)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        return tf.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "layer = CustomEmbedding(10, 32, mask_zero=True)\n",
    "x = np.random.random((3, 10)) * 9\n",
    "x = x.astype(\"int32\")\n",
    "\n",
    "y = layer(x)\n",
    "mask = layer.compute_mask(x)\n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb34149eb837"
   },
   "source": [
    "## 호환 가능한 레이어에서 전파를 마스크하도록 선택\n",
    "\n",
    "대부분의 레이어는 시간 차원을 수정하지 않으므로 현재 마스크를 수정할 필요가 없습니다. 그러나 그들은 여전히 현재 마스크를 변경하지 않고 다음 레이어로 **전파** 할 수 있기를 원할 수 있습니다. **이것은 옵트 인 동작입니다.** 기본적으로 사용자 정의 레이어는 현재 마스크를 제거합니다 (프레임 워크에서 마스크 전파가 안전한지 여부를 알 수있는 방법이 없기 때문에).\n",
    "\n",
    "시간 차원을 수정하지 않는 사용자 정의 레이어가 있고 현재 입력 마스크를 전파 할 수 있으려면 레이어 생성자에서 `self.supports_masking = True` 를 설정해야합니다. 이 경우 `compute_mask()` 의 기본 동작은 현재 마스크를 통과하는 것입니다.\n",
    "\n",
    "마스크 전파가 허용 된 레이어의 예는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T02:09:10.134015Z",
     "iopub.status.busy": "2021-01-15T02:09:10.133327Z",
     "iopub.status.idle": "2021-01-15T02:09:10.135857Z",
     "shell.execute_reply": "2021-01-15T02:09:10.135231Z"
    },
    "id": "895c35534d06"
   },
   "outputs": [],
   "source": [
    "class MyActivation(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyActivation, self).__init__(**kwargs)\n",
    "        # Signal that the layer is safe for mask propagation\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2e1e0a81995"
   },
   "source": [
    "이제 마스크를 생성하는 레이어(예: `Embedding`)와 마스크를 소비하는 레이어(예: `LSTM`) 사이에서 이 사용자 정의 레이어를 사용하여 마스크 소비 레이어까지 마스크를 전달할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T02:09:10.144141Z",
     "iopub.status.busy": "2021-01-15T02:09:10.143457Z",
     "iopub.status.idle": "2021-01-15T02:09:11.045062Z",
     "shell.execute_reply": "2021-01-15T02:09:11.044477Z"
    },
    "id": "486e39e9a9a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask found: KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.bool, name=None), name='Placeholder_1:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
    "x = MyActivation()(x)  # Will pass the mask along\n",
    "print(\"Mask found:\", x._keras_mask)\n",
    "outputs = layers.LSTM(32)(x)  # Will receive the mask\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55ab9c02f4ba"
   },
   "source": [
    "## 마스크 정보가 필요한 레이어 작성\n",
    "\n",
    "일부 레이어는 마스크 *소비자*입니다. 이러한 레이어는 `call`에서 `mask` 인수를 허용하고 이를 사용하여 특정 타임스텝을 건너뛸지 여부를 결정합니다.\n",
    "\n",
    "이러한 계층을 작성하려면 `call` 서명에 `mask=None` 인수를 추가하면됩니다. 입력과 관련된 마스크는 가능할 때마다 레이어로 전달됩니다.\n",
    "\n",
    "다음은 간단한 예입니다. 마스크 된 시간 단계를 버리고 입력 시퀀스의 시간 차원 (축 1)에 대한 소프트 맥스를 계산하는 레이어입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-15T02:09:11.055491Z",
     "iopub.status.busy": "2021-01-15T02:09:11.054617Z",
     "iopub.status.idle": "2021-01-15T02:09:11.140194Z",
     "shell.execute_reply": "2021-01-15T02:09:11.139589Z"
    },
    "id": "7809df539882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask found: KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.bool, name=None), name='NotEqual:0')\n",
      "Mask found: KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.bool, name=None), name='Placeholder_1:0')\n",
      "(None, None, 1)\n",
      "(None, None, 1)\n",
      "(32, 100, 1)\n",
      "(32, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "class TemporalSoftmax(keras.layers.Layer):\n",
    "    def call(self, inputs, mask=None):\n",
    "        print(inputs.shape)\n",
    "        \n",
    "        broadcast_float_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n",
    "        print(broadcast_float_mask.shape)\n",
    "        inputs_exp = tf.exp(inputs) * broadcast_float_mask\n",
    "        inputs_sum = tf.reduce_sum(inputs * broadcast_float_mask, axis=1, keepdims=True)\n",
    "        return inputs_exp / inputs_sum\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "x = layers.Embedding(input_dim=10, output_dim=32, mask_zero=True)(inputs)\n",
    "print(\"Mask found:\", x._keras_mask)\n",
    "x = layers.Dense(1)(x)\n",
    "print(\"Mask found:\", x._keras_mask)\n",
    "outputs = TemporalSoftmax()(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "y = model(np.random.randint(0, 10, size=(32, 100)), np.random.random((32, 100, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6373f43bbe18"
   },
   "source": [
    "## 요약\n",
    "\n",
    "이것이 Keras의 패딩 및 마스킹에 대해 알아야 할 전부입니다. 요약하자면:\n",
    "\n",
    "- \"Masking\"은 레이어가 시퀀스 입력에서 특정 시간 단계를 건너 뛰거나 무시할 때를 알 수있는 방법입니다.\n",
    "- 일부 레이어는 마스크 생성기입니다. `Embedding` 하면 입력 값에서 마스크를 생성 할 수 있으며 ( `mask_zero=True` ) `Masking` 레이어도 생성 할 수 있습니다.\n",
    "- 일부 레이어는 마스크 소비자입니다. `__call__` 메서드에 `mask` 인수를 표시합니다. 이것은 RNN 계층의 경우입니다.\n",
    "- Functional API 및 Sequential API에서 마스크 정보는 자동으로 전파됩니다.\n",
    "- 독립형 방식으로 레이어를 사용하는 경우 `mask` 인수를 레이어에 수동으로 전달할 수 있습니다.\n",
    "- 현재 마스크를 수정하거나 새 마스크를 생성하거나 입력과 관련된 마스크를 사용하는 레이어를 쉽게 작성할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "masking_and_padding.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
