{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컨볼루션 층 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import Convolution\n",
    "\n",
    "image = np.arange(16).reshape(1,1,4,4)\n",
    "print(image.shape)\n",
    "print(image)\n",
    "\n",
    "W = np.ones((1,1,2,2))\n",
    "print(W)\n",
    "b = np.full((1,), 3)\n",
    "print(b.shape)\n",
    "print(b)\n",
    "conv = Convolution(W, b)\n",
    "out = conv.forward(image)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다차원 배열의 연산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.arange(12).reshape(2,2,3)\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(2,2,3)\n",
    "b = a.transpose(2, 1, 0)\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(2,2,3)\n",
    "b = a.transpose(1, 0, 2)\n",
    "print(b.shape) # (2,2,3)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(2,2,3)\n",
    "b = a.transpose(0, 2, 1)\n",
    "print(b.shape) # (2,3,2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(2,2,3)\n",
    "b = a.transpose(2, 0, 1)\n",
    "print(b.shape) \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### im2col 함수 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.arange(9).reshape(1, 1, 3, 3)  # OH = (H - FH + 1) , OW = (W - FW + 1) \n",
    "col1 = im2col(x1, 2, 2)\n",
    "print(col1.shape)  # (N*OH*OW, C*FH*FW)\n",
    "# print(col1)\n",
    "\n",
    "x2 = np.arange(18).reshape(1, 2, 3, 3)\n",
    "col2 = im2col(x2, 2, 2)\n",
    "print(col2.shape)\n",
    "# print(col2)\n",
    "\n",
    "x3 = np.arange(36).reshape(2, 2, 3, 3)\n",
    "col3 = im2col(x3, 2, 2)\n",
    "print(col3.shape)\n",
    "# print(col3)\n",
    "\n",
    "x4 = np.arange(49).reshape(1, 1, 7, 7)\n",
    "col4 = im2col(x4, 5, 5)\n",
    "print(col4.shape)   # ( N*OH*OW, C*FH*FW)\n",
    "\n",
    "x5 = np.arange(10*3*7*7).reshape(10, 3, 7, 7)\n",
    "col5 = im2col(x5, 5, 5)\n",
    "print(col5.shape)   # ( N*OH*OW, C*FH*FW)\n",
    "\n",
    "x6 = np.arange(3*2*7*7).reshape(3, 2, 7, 7)\n",
    "col5 = im2col(x6, 5, 5)\n",
    "print(col5.shape)   # ( N*OH*OW, C*FH*FW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.pad() 의 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pad=0\n",
    "input_data = np.arange(16).reshape((1,1,4,4))\n",
    "print(input_data)\n",
    "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬의 슬라이스 문법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "print(a[0])\n",
    "print(a[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "print(a[0])   # [1 2]\n",
    "print(a[0:1]) # [[1 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(9).reshape(1,1,3,3)\n",
    "print(a.shape)  # (1, 1, 3, 3)\n",
    "a = a[:, :, 0:2:1, 0:2:1]\n",
    "print(a.shape) #  (1, 1, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(36).reshape(1,1,2,2,3,3)\n",
    "print(a.shape)  # (1, 1, 2, 2, 3, 3)\n",
    "a = a[:, :, 0, 0, :, :]\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.arange(16).reshape(1,1,4,4)\n",
    "col = np.zeros(36).reshape(1,1,2,2,3,3)\n",
    "\n",
    "col[:, :, 0, 0, :, :] = image[:, :, 0:3:1, 0:3:1]\n",
    "print(col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input 채널이 1개인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "pad=0\n",
    "stride=1\n",
    "filter_h = 2\n",
    "filter_w = 2\n",
    "input_data = np.arange(16).reshape(1,1,4,4)\n",
    "# print(input_data.shape)\n",
    "# print(input_data)\n",
    "\n",
    "N, C, H, W = input_data.shape\n",
    "out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "print(img.shape)\n",
    "print(img)\n",
    "print(col.shape)\n",
    "\n",
    "# for y in range(filter_h):\n",
    "#     y_max = y + stride*out_h\n",
    "#     for x in range(filter_w):\n",
    "#         x_max = x + stride*out_w\n",
    "#         col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "# print(col)    # (1,1,2,2,3,3)            \n",
    "\n",
    "col[:, :, 0, 0, :, :] = img[:, :, 0:3:1, 0:3:1]\n",
    "col[:, :, 0, 1, :, :] = img[:, :, 0:3:1, 1:4:1]\n",
    "col[:, :, 1, 0, :, :] = img[:, :, 1:4:1, 0:3:1]\n",
    "col[:, :, 1, 1, :, :] = img[:, :, 1:4:1, 1:4:1]\n",
    "\n",
    "print(col)\n",
    "ret = col.transpose(0, 4, 5, 1, 2, 3)\n",
    "# print(ret.shape)  # (1, 3, 3, 1, 2, 2)\n",
    "# print(ret)\n",
    "col = ret.reshape( N*out_h*out_w, -1 )\n",
    "print(col.shape)\n",
    "print(col)\n",
    "\n",
    "W = np.ones((1,1,2,2))\n",
    "print(W)\n",
    "col_W = W.reshape(1, -1)\n",
    "print(col_W.shape)\n",
    "print(col_W)\n",
    "col_W = col_W.T\n",
    "print(col_W.shape)\n",
    "print(col_W)\n",
    "\n",
    "b = np.full((1,), 3)\n",
    "out = np.dot(col, col_W) + b  # (9,4)(4,1)\n",
    "print( out.shape )\n",
    "print(out)\n",
    "out = out.reshape(N, out_h, out_w, -1)\n",
    "print( out.shape )\n",
    "print(out)\n",
    "out = out.transpose(0, 3, 1, 2)\n",
    "print( out.shape )\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 테스트(채널이 1개인 경우)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.layers import Convolution\n",
    "\n",
    "image = np.arange(16).reshape(1,1,4,4)\n",
    "weight = np.ones((1,1,2,2))\n",
    "b = np.full((1,),3)\n",
    "print(image)\n",
    "print(weight)\n",
    "print(b)\n",
    "conv = Convolution(weight, b)\n",
    "cout = conv.forward(image)\n",
    "print(cout)\n",
    "print(cout.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input 채널이 2개인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "pad=0\n",
    "stride=1\n",
    "filter_h = 2\n",
    "filter_w = 2\n",
    "input_data = np.arange(32).reshape(1,2,4,4)\n",
    "\n",
    "N, C, H, W = input_data.shape\n",
    "out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "for y in range(filter_h):\n",
    "    y_max = y + stride*out_h\n",
    "    for x in range(filter_w):\n",
    "        x_max = x + stride*out_w\n",
    "        col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "print(col.shape)  # (1,2,2,2,3,3)\n",
    "print(col)        \n",
    "        \n",
    "ret = col.transpose(0, 4, 5, 1, 2, 3)\n",
    "print(ret.shape)  # (1, 3, 3, 2, 2, 2)\n",
    "print(ret)\n",
    "\n",
    "col = ret.reshape( N*out_h*out_w, -1 )\n",
    "print(col.shape)\n",
    "print(col)\n",
    "\n",
    "W = np.ones((N,C,filter_h,filter_w))\n",
    "col_W = W.reshape(N, -1).T\n",
    "print(col_W.shape)\n",
    "print(col_W )\n",
    "out = np.dot(col, col_W)  # (9,8)(8,1)\n",
    "print(out )\n",
    "out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 테스트(채널이 2개인 경우)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.layers import Convolution\n",
    "\n",
    "image = np.arange(32).reshape(1,2,4,4)\n",
    "weight = np.ones((1,2,2,2))\n",
    "b = np.zeros((1,))\n",
    "print(image)\n",
    "print(weight)\n",
    "print(b)\n",
    "conv = Convolution(weight, b)\n",
    "cout = conv.forward(image)\n",
    "print(cout)\n",
    "print(cout.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input 그림이 1개, 채널이 1개, 필터가 2개인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "pad=0\n",
    "stride=1\n",
    "filter_h = 2\n",
    "filter_w = 2\n",
    "input_data = np.arange(16).reshape(1,1,4,4)\n",
    "\n",
    "N, C, H, W = input_data.shape\n",
    "out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "for y in range(filter_h):\n",
    "    y_max = y + stride*out_h\n",
    "    for x in range(filter_w):\n",
    "        x_max = x + stride*out_w\n",
    "        col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "ret = col.transpose(0, 4, 5, 1, 2, 3)\n",
    "# print(ret.shape)  # (1, 3, 3, 2, 2, 2)\n",
    "# print(ret)\n",
    "col = ret.reshape( N*out_h*out_w, -1 )\n",
    "# print(col.shape)\n",
    "# print(col)\n",
    "\n",
    "FN=2\n",
    "\n",
    "W = np.array([[[[1,1],\n",
    "                [1,1]]],\n",
    "              [[[2,2],\n",
    "                [2,2]]]])\n",
    "print(W.shape)\n",
    "col_W = W.reshape(FN, -1)\n",
    "print(col_W.shape)\n",
    "col_W = col_W.T\n",
    "print(col_W.shape)\n",
    "b = np.full((1,),3)\n",
    "out = np.dot(col, col_W) + b # (9,4)(4,2)\n",
    "out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.layers import Convolution\n",
    "\n",
    "image = np.arange(16).reshape(1,1,4,4)\n",
    "weight = np.array([[[[1,1],\n",
    "                     [1,1]]],\n",
    "                   [[[2,2],\n",
    "                     [2,2]]]])\n",
    "print(weight.shape)\n",
    "b = np.full((1,),3)\n",
    "conv = Convolution(weight, b)\n",
    "cout = conv.forward(image)\n",
    "print(cout.shape)  # (1,2,3,3)\n",
    "print(cout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input 그림이 1개, 채널이 2개, 필터가 3개인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.layers import Convolution\n",
    "\n",
    "image = np.arange(32).reshape(1,2,4,4)\n",
    "weight = np.array([\n",
    "                   [[[1,1],[1,1]],[[1,1],[1,1]]],\n",
    "                   [[[2,2],[2,2]],[[2,2],[2,2]]],\n",
    "                   [[[3,3],[3,3]],[[3,3],[3,3]]]\n",
    "                  ])\n",
    "print(weight.shape)  # (3,2,2,2)\n",
    "b = np.full((1,), 3)\n",
    "conv = Convolution(weight, b)\n",
    "cout = conv.forward(image)\n",
    "print(cout.shape)  # (1,3,3,3)\n",
    "print(cout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "pad=0\n",
    "stride=1\n",
    "filter_h = 2\n",
    "filter_w = 2\n",
    "input_data = np.arange(32).reshape(1,2,4,4)\n",
    "\n",
    "N, C, H, W = input_data.shape\n",
    "out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "for y in range(filter_h):\n",
    "    y_max = y + stride*out_h\n",
    "    for x in range(filter_w):\n",
    "        x_max = x + stride*out_w\n",
    "        col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "ret = col.transpose(0, 4, 5, 1, 2, 3)\n",
    "# print(ret.shape)  # (1, 3, 3, 2, 2, 2)\n",
    "# print(ret)\n",
    "col = ret.reshape( N*out_h*out_w, -1 )\n",
    "# print(col.shape)\n",
    "# print(col)\n",
    "\n",
    "FN=3\n",
    "\n",
    "W =       np.array([\n",
    "                   [[[1,1],[1,1]],[[1,1],[1,1]]],\n",
    "                   [[[2,2],[2,2]],[[2,2],[2,2]]],\n",
    "                   [[[3,3],[3,3]],[[3,3],[3,3]]]\n",
    "                  ])\n",
    "print(W.shape)\n",
    "col_W = W.reshape(FN, -1)\n",
    "print(col_W.shape)\n",
    "col_W = col_W.T\n",
    "print(col_W.shape)\n",
    "out = np.dot(col, col_W)  # (9,8)(8,3)\n",
    "out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "print(out.shape)  # (1,3,3,3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input 그림이 2개, 채널이 2개, 필터가 3개인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "pad=0\n",
    "stride=1\n",
    "filter_h = 2\n",
    "filter_w = 2\n",
    "input_data = np.arange(64).reshape(2,2,4,4)\n",
    "\n",
    "N, C, H, W = input_data.shape\n",
    "out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "for y in range(filter_h):\n",
    "    y_max = y + stride*out_h\n",
    "    for x in range(filter_w):\n",
    "        x_max = x + stride*out_w\n",
    "        col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "ret = col.transpose(0, 4, 5, 1, 2, 3)\n",
    "# print(ret.shape)  # (1, 3, 3, 2, 2, 2)\n",
    "# print(ret)\n",
    "col = ret.reshape( N*out_h*out_w, -1 )\n",
    "# print(col.shape)\n",
    "# print(col)\n",
    "\n",
    "FN=3\n",
    "\n",
    "W =       np.array([\n",
    "                   [[[1,1],[1,1]],[[1,1],[1,1]]],\n",
    "                   [[[2,2],[2,2]],[[2,2],[2,2]]],\n",
    "                   [[[3,3],[3,3]],[[3,3],[3,3]]]\n",
    "                  ])\n",
    "print(W.shape)\n",
    "col_W = W.reshape(FN, -1)\n",
    "print(col_W.shape)\n",
    "col_W = col_W.T\n",
    "print(col_W.shape)\n",
    "out = np.dot(col, col_W)  # (18,8)(8,3)\n",
    "out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "print(out.shape)  # (2,3,3,3)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.layers import Convolution\n",
    "\n",
    "input_data = np.arange(64).reshape(2,2,4,4)\n",
    "weight =       np.array([\n",
    "                   [[[1,1],[1,1]],[[1,1],[1,1]]],\n",
    "                   [[[2,2],[2,2]],[[2,2],[2,2]]],\n",
    "                   [[[3,3],[3,3]],[[3,3],[3,3]]]\n",
    "                  ])\n",
    "print(weight.shape)\n",
    "b = np.full((1,),3)\n",
    "conv = Convolution(weight, b)\n",
    "cout = conv.forward(input_data)\n",
    "print(cout.shape)  # (2,3,3,3)\n",
    "print(cout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  합성곱 연산의 정확한 고찰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "filter_h=2\n",
    "filter_w=2\n",
    "out_h=3\n",
    "out_w=3\n",
    "input_data = np.arange(16).reshape((1,1,4,4))\n",
    "print(input_data.shape)\n",
    "col = np.zeros((1, 1, filter_h, filter_w, out_h, out_w))\n",
    "# col = np.zeros((1, 1, out_h, out_w, filter_h, filter_w ))\n",
    "print(col.shape)\n",
    "\n",
    "# for y in range(out_h):\n",
    "#     y_max = y + filter_h\n",
    "#     for x in range(out_w):\n",
    "#         x_max = x + filter_w\n",
    "#         col[:, :, y, x, :, :] = input_data[:, :, y:y_max, x:x_max]\n",
    "\n",
    "for y in range(filter_h):\n",
    "    y_max = y + out_h\n",
    "    for x in range(filter_w):\n",
    "        x_max = x + out_w\n",
    "        col[:, :, y, x, :, :] = input_data[:, :, y:y_max, x:x_max]\n",
    "\n",
    "\n",
    "print(col)\n",
    "ret = col.transpose(0, 4, 5, 1, 2, 3)\n",
    "print(ret)\n",
    "ret1 = ret.reshape( 3*3, -1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad를 사용했을 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.layers import Convolution\n",
    "\n",
    "image = np.arange(1,17).reshape(1,1,4,4)\n",
    "weight = np.ones((1,1,3,3))\n",
    "b = np.full((1,),3)\n",
    "print(image)\n",
    "print(weight)\n",
    "print(b)\n",
    "conv = Convolution(weight, b, pad=1)\n",
    "cout = conv.forward(image)\n",
    "print(cout)\n",
    "print(cout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "pad=1\n",
    "stride=1\n",
    "filter_h = 2\n",
    "filter_w = 2\n",
    "input_data = np.arange(9).reshape(1,1,3,3)\n",
    "print(input_data)\n",
    "\n",
    "N, C, H, W = input_data.shape\n",
    "out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "print(img)\n",
    "\n",
    "col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "for y in range(filter_h):\n",
    "    y_max = y + stride*out_h\n",
    "    for x in range(filter_w):\n",
    "        x_max = x + stride*out_w\n",
    "        col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "ret = col.transpose(0, 4, 5, 1, 2, 3)\n",
    "# print(ret.shape)  # (1, 3, 3, 2, 2, 2)\n",
    "# print(ret)\n",
    "col = ret.reshape( N*out_h*out_w, -1 )\n",
    "# print(col.shape)\n",
    "# print(col)\n",
    "\n",
    "FN=1\n",
    "\n",
    "W = np.array([[[[1,1],[1,1]]]])\n",
    "print(W.shape)\n",
    "\n",
    "b = np.full((1,), 3)\n",
    "col_W = W.reshape(FN, -1)\n",
    "print(col_W.shape)\n",
    "col_W = col_W.T\n",
    "print(col_W.shape)\n",
    "out = np.dot(col, col_W)  # (16,4)(4,1)\n",
    "print(out)\n",
    "out = out + b  \n",
    "print(out)\n",
    "out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "print(out.shape)  # (1,1,4,4)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱의 미분 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "pad=0\n",
    "stride=1\n",
    "filter_h = 2\n",
    "filter_w = 2\n",
    "input_data = np.arange(16).reshape(1,1,4,4)\n",
    "\n",
    "N, C, H, W = input_data.shape\n",
    "out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "for y in range(filter_h):\n",
    "    y_max = y + stride*out_h\n",
    "    for x in range(filter_w):\n",
    "        x_max = x + stride*out_w\n",
    "        col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "ret = col.transpose(0, 4, 5, 1, 2, 3)\n",
    "col = ret.reshape( N*out_h*out_w, -1 )\n",
    "\n",
    "W = np.ones((1,1,2,2))\n",
    "col_W = W.reshape(1, -1)\n",
    "col_W = col_W.T\n",
    "\n",
    "b = np.full((1,), 3)\n",
    "out = np.dot(col, col_W) + b  # (9,4)(4,1)\n",
    "out = out.reshape(N, out_h, out_w, -1)\n",
    "out = out.transpose(0, 3, 1, 2)\n",
    "print( out.shape )\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = 1\n",
    "dout = np.arange(9).reshape(1,1,3,3)\n",
    "# print(dout.shape)\n",
    "dout = dout.transpose(0,2,3,1)\n",
    "# print(dout.shape)\n",
    "dout = dout.reshape(-1, FN)\n",
    "# print(dout.shape)\n",
    "# print(dout)\n",
    "db = np.sum(dout, axis=0)\n",
    "# print(db)\n",
    "dW = np.dot(col.T, dout)\n",
    "dW = dW.transpose(1, 0).reshape(1, 1, 2, 2)\n",
    "# print(dW)\n",
    "\n",
    "dcol = np.dot(dout, col_W.T)\n",
    "# print(dcol)\n",
    "\n",
    "N, C, H, W = input_data.shape\n",
    "out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "dcol = dcol.reshape(N, out_h, out_w, C, filter_h, filter_w)\n",
    "# print(dcol)\n",
    "dcol = dcol.transpose(0, 3, 4, 5, 1, 2)\n",
    "# print(dcol)\n",
    "\n",
    "img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "\n",
    "img[:, :, 0:3:1, 0:3:1] += dcol[:, :, 0, 0, :, :] \n",
    "print(img)\n",
    "img[:, :, 0:3:1, 1:4:1] += dcol[:, :, 0, 1, :, :] \n",
    "print(img)\n",
    "img[:, :, 1:4:1, 0:3:1] += dcol[:, :, 1, 0, :, :] \n",
    "print(img)\n",
    "img[:, :, 1:4:1, 1:4:1] += dcol[:, :, 1, 1, :, :] \n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.util import im2col, col2im\n",
    "\n",
    "input_data = np.arange(16).reshape((1,1,4,4))\n",
    "print(input_data)\n",
    "col = im2col(input_data, 2, 2)\n",
    "print(col.shape)\n",
    "print(col)\n",
    "image = my_col2im(col, input_data.shape, 2, 2)\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.layers import Convolution\n",
    "\n",
    "image = np.arange(16).reshape(1,1,4,4)\n",
    "weight = np.ones((1,1,2,2))\n",
    "b = np.full((1,),3)\n",
    "print(image)\n",
    "print(weight)\n",
    "print(b)\n",
    "conv = Convolution(weight, b)\n",
    "cout = conv.forward(image)\n",
    "print(cout)\n",
    "print(cout.shape)\n",
    "dout = np.arange(9).reshape(1,1,3,3)\n",
    "dx = conv.backward(dout)\n",
    "print(dx)\n",
    "print(dx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Convolution test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image 채널이 2개인 경우 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "sys.path.append(os.pardir)\n",
    "from common.layers import Convolution\n",
    "\n",
    "image = np.arange(32).reshape(1,2,4,4)\n",
    "weight = np.ones((1,2,2,2))\n",
    "b = np.full((1,),3)\n",
    "print(image)\n",
    "print(weight)\n",
    "print(b)\n",
    "conv = Convolution(weight, b)\n",
    "cout = conv.forward(image)\n",
    "print(cout)\n",
    "print(cout.shape)\n",
    "dout = np.arange(9).reshape(1,1,3,3)\n",
    "dx = conv.backward(dout)\n",
    "print(dx)\n",
    "print(dx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image 필터가  2개인 경우 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "FN=2\n",
    "pad=0\n",
    "stride=1\n",
    "filter_h = 2\n",
    "filter_w = 2\n",
    "input_data = np.arange(16).reshape(1,1,4,4)\n",
    "\n",
    "N, C, H, W = input_data.shape\n",
    "out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "for y in range(filter_h):\n",
    "    y_max = y + stride*out_h\n",
    "    for x in range(filter_w):\n",
    "        x_max = x + stride*out_w\n",
    "        col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "ret = col.transpose(0, 4, 5, 1, 2, 3)\n",
    "col = ret.reshape( N*out_h*out_w, -1 )\n",
    "\n",
    "W = np.ones((FN,1,2,2))\n",
    "col_W = W.reshape(FN, -1)\n",
    "col_W = col_W.T\n",
    "\n",
    "b = np.full((1,), 3)\n",
    "out = np.dot(col, col_W) + b  # (9,4)(4,1)\n",
    "out = out.reshape(N, out_h, out_w, -1)\n",
    "out = out.transpose(0, 3, 1, 2)\n",
    "print( out.shape )\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = np.arange(18).reshape(1,FN,3,3)\n",
    "# print(dout.shape)\n",
    "dout = dout.transpose(0,2,3,1)\n",
    "# print(dout.shape)\n",
    "dout = dout.reshape(-1, FN)\n",
    "# print(dout.shape)\n",
    "# print(dout)\n",
    "db = np.sum(dout, axis=0)\n",
    "# print(db)\n",
    "dW = np.dot(col.T, dout)\n",
    "dW = dW.transpose(1, 0).reshape(FN, 1, 2, 2)\n",
    "print(dW)\n",
    "\n",
    "dcol = np.dot(dout, col_W.T)\n",
    "# print(dcol)\n",
    "\n",
    "N, C, H, W = input_data.shape\n",
    "out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "dcol = dcol.reshape(N, out_h, out_w, C, filter_h, filter_w)\n",
    "# print(dcol)\n",
    "dcol = dcol.transpose(0, 3, 4, 5, 1, 2)\n",
    "# print(dcol)\n",
    "\n",
    "img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "\n",
    "img[:, :, 0:3:1, 0:3:1] += dcol[:, :, 0, 0, :, :] \n",
    "print(img)\n",
    "img[:, :, 0:3:1, 1:4:1] += dcol[:, :, 0, 1, :, :] \n",
    "print(img)\n",
    "img[:, :, 1:4:1, 0:3:1] += dcol[:, :, 1, 0, :, :] \n",
    "print(img)\n",
    "img[:, :, 1:4:1, 1:4:1] += dcol[:, :, 1, 1, :, :] \n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀링 소스 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.util import im2col\n",
    "from common.util import col2im\n",
    "from common.layers import Convolution\n",
    "\n",
    "class MyPooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        print('col', col)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        print('col', col)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        print('arg_max', arg_max)\n",
    "        out = np.max(col, axis=1)\n",
    "        print('out', out)\n",
    "        out = out.reshape(N, out_h, out_w, C)\n",
    "        print('out', out)\n",
    "        out = out.transpose(0, 3, 1, 2)\n",
    "        print('out', out)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        print('dmax', dmax)\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        print('dcol', dcol)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 채널이 1개인 경우 맥스 풀링 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[[ 7, 11, 13, 15],\n",
    "                [ 3,  4,  2,  3],\n",
    "                [ 1,  2, 17,  9],\n",
    "                [ 1,  8,  3, 10]]]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = MyPooling(2,2,2)\n",
    "pout = pool.forward(x)\n",
    "print(pout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 채널이 2개인 경우 맥스 풀링 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[[ 7, 11, 13, 15],\n",
    "                [ 3,  4,  2,  3],\n",
    "                [ 1,  2, 17,  9],\n",
    "                [ 1,  8,  3, 10]],\n",
    "               [[ 5,  8,  6,  7],\n",
    "                [ 10, 4, 11, 13],\n",
    "                [ 9,  3, 10,  4],\n",
    "                [ 1,  2,  5, 16]],\n",
    "              ]])\n",
    "print(x.shape)\n",
    "pool = MyPooling(2,2,2)\n",
    "pout = pool.forward(x)\n",
    "print(pout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 맥스 풀링 미분 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dout = np.array([[[[1,2],\n",
    "                   [3,4]],\n",
    "                  [[5,6],\n",
    "                   [7,8]]]])\n",
    "print(dout.shape)\n",
    "dout = dout.transpose(0, 2, 3, 1)\n",
    "print(dout)\n",
    "print(len(dout))\n",
    "print(dout.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_h=2\n",
    "pool_w=2\n",
    "pool_size = pool_h * pool_w\n",
    "dmax = np.zeros((dout.size, pool_size))\n",
    "print(dmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_max = np.array([1,2,1,3,3,0,0,3])\n",
    "print(dout.flatten())\n",
    "print(np.arange(8))\n",
    "print(arg_max.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmax[np.arange(arg_max.size), arg_max.flatten()] = dout.flatten()\n",
    "# dmax[[0,1,2,3,4,5,6,7], [1,2,1,3,3,0,0,3]] = [0,4,1,5,2,6,3,7]\n",
    "print(dmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]+[4,5]\n",
    "print(a)\n",
    "b = \"hello\"+\"world\"\n",
    "print(b)\n",
    "c = (1,2,2,1)+(4,)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmax = dmax.reshape(dout.shape + (pool_size,))  # (1,2,2,1) + (4) => (1, 2, 2, 1, 4)\n",
    "print(dmax.shape)\n",
    "print(dmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)  # (1,2,2,2,4)\n",
    "print(dcol.shape)\n",
    "print(dcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = my_col2im(dcol, (1,1,4,4), 2,2,2,0)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dout = np.array([[[[1,2],\n",
    "                   [3,4]]]])\n",
    "dx = pool.backward(dout)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 채널이 2개인 경우 맥스 풀링 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[[ 7, 11, 13, 15],\n",
    "                [ 3,  4,  2,  3],\n",
    "                [ 1,  2, 17,  9],\n",
    "                [ 1,  8,  3, 10]],\n",
    "               [[ 5,  8,  6,  7],\n",
    "                [10,  4, 11, 13],\n",
    "                [ 9,  3, 10,  4],\n",
    "                [ 1,  2,  5, 16]]\n",
    "              ]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = MyPooling(2,2,2)\n",
    "out = pool.forward(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = np.array([[[[1,2],\n",
    "                   [3,4]],\n",
    "                  [[5,6],\n",
    "                   [7,8]]]])\n",
    "print(dout.shape)\n",
    "dx = pool.backward(dout)\n",
    "print(dx.shape)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "from matplotlib.image import imread\n",
    "from common.layers import Convolution\n",
    "\n",
    "def filter_show(filters, nx=4, show_num=16):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(show_num / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(show_num):\n",
    "        ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "# 학습된 가중치\n",
    "network.load_params(\"params.pkl\")\n",
    "\n",
    "filter_show(network.params['W1'], 16)\n",
    "\n",
    "img = imread('../dataset/cactus_gray.png')\n",
    "# print(img.shape)\n",
    "img = img.reshape(1, 1, *img.shape)\n",
    "# print(img.shape)\n",
    "fig = plt.figure()\n",
    "\n",
    "# w_idx = 1\n",
    "\n",
    "# w = network.params['W1']\n",
    "# b = network.params['b1']\n",
    "# print(w.shape)\n",
    "# print(b.shape)\n",
    "# conv_layer = Convolution(w, b) \n",
    "# out = conv_layer.forward(img)\n",
    "# print(out.shape)  # (1,30,329,246)\n",
    "\n",
    "# print(network.params['W1'].shape)\n",
    "for i in range(16):\n",
    "    w = network.params['W1'][i]\n",
    "    b = 0  # network.params['b1'][i]\n",
    "\n",
    "    w = w.reshape(1, *w.shape)\n",
    "    #b = b.reshape(1, *b.shape)\n",
    "    conv_layer = Convolution(w, b) \n",
    "    out = conv_layer.forward(img)\n",
    "#     print(out.shape)\n",
    "    out = out.reshape(out.shape[2], out.shape[3])\n",
    "    \n",
    "    ax = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(out, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from simple_convnet import SimpleConvNet\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,10, 10), \n",
    "                        conv_param = {'filter_num':10, 'filter_size':3, 'pad':0, 'stride':1},\n",
    "                        hidden_size=10, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "X = np.random.rand(100).reshape((1, 1, 10, 10))\n",
    "T = np.array([1]).reshape((1,1))\n",
    "\n",
    "grad_num = network.numerical_gradient(X, T)\n",
    "grad = network.gradient(X, T)\n",
    "\n",
    "for key, val in grad_num.items():\n",
    "    print(key, np.abs(grad_num[key] - grad[key]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        print(self.params['W2'].shape)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()                                                  # (100,1,28,28)\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])  # (100,30,24,24)\n",
    "        self.layers['Relu1'] = Relu()                                                # (100,30,24,24)\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)                 # (100,30,12,12)    \n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])        # (100,4320)(4320,100)\n",
    "        self.layers['Relu2'] = Relu()                                                # (100,100)\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])        # (100,100)(100,10)\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()                                          # (100,10)\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "#             print(x.shape)\n",
    "            x = layer.forward(x)\n",
    "\n",
    "#         print(x.shape)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4320, 100)\n",
      "train loss:2.2989340417785127\n",
      "=== epoch:1, train acc:0.132, test acc:0.123 ===\n",
      "train loss:2.2964891367342615\n",
      "train loss:2.2927044889058767\n",
      "train loss:2.2892615481466523\n",
      "train loss:2.2769012442819503\n",
      "train loss:2.2747264015938056\n",
      "train loss:2.24553445407179\n",
      "train loss:2.2403654206425814\n",
      "train loss:2.221265588470099\n",
      "train loss:2.2070089283038867\n",
      "train loss:2.1690259928265148\n",
      "train loss:2.1155424826093046\n",
      "train loss:2.0914410007713724\n",
      "train loss:2.052212171155078\n",
      "train loss:1.970225603166154\n",
      "train loss:1.9260155269490633\n",
      "train loss:1.7757111038802398\n",
      "train loss:1.7773112603862162\n",
      "train loss:1.624612415489894\n",
      "train loss:1.6027247167370344\n",
      "train loss:1.5948388900682853\n",
      "train loss:1.43406919465969\n",
      "train loss:1.3403106667824403\n",
      "train loss:1.4077817206179861\n",
      "train loss:1.375172016302764\n",
      "train loss:1.1077763501615947\n",
      "train loss:1.0757165610286414\n",
      "train loss:1.1381505336527018\n",
      "train loss:0.9220860712128566\n",
      "train loss:0.8931791889208733\n",
      "train loss:0.9058758756278901\n",
      "train loss:0.8664728531593169\n",
      "train loss:0.6966598799911553\n",
      "train loss:0.7607144969463184\n",
      "train loss:0.6591203685762381\n",
      "train loss:0.8213878962708948\n",
      "train loss:0.749445335265723\n",
      "train loss:0.6135814589569494\n",
      "train loss:0.5857106072285423\n",
      "train loss:0.6896580034916768\n",
      "train loss:0.6332682206228302\n",
      "train loss:0.7342981485500267\n",
      "train loss:0.6745511803114271\n",
      "train loss:0.6324575343654825\n",
      "train loss:0.5856322705758494\n",
      "train loss:0.5090699040970973\n",
      "train loss:0.6685960710766417\n",
      "train loss:0.633454590527287\n",
      "train loss:0.47210688442877546\n",
      "train loss:0.5412328913115111\n",
      "train loss:0.36062449164490146\n",
      "=== epoch:2, train acc:0.781, test acc:0.765 ===\n",
      "train loss:0.5505183666599873\n",
      "train loss:0.5970895956434138\n",
      "train loss:0.5133029130634319\n",
      "train loss:0.5583462974705298\n",
      "train loss:0.45031703154501135\n",
      "train loss:0.30115041087516453\n",
      "train loss:0.4005858485169631\n",
      "train loss:0.4983384319905992\n",
      "train loss:0.5460239554450115\n",
      "train loss:0.4977385063716232\n",
      "train loss:0.5702630408920207\n",
      "train loss:0.548132783818348\n",
      "train loss:0.5350503532278621\n",
      "train loss:0.6216228009411696\n",
      "train loss:0.291524324659976\n",
      "train loss:0.4309209254514225\n",
      "train loss:0.5008130719873066\n",
      "train loss:0.33028589429532246\n",
      "train loss:0.5863995048467189\n",
      "train loss:0.3689814695832513\n",
      "train loss:0.5738760863659143\n",
      "train loss:0.34404954270971033\n",
      "train loss:0.5360437876428608\n",
      "train loss:0.3470616936891361\n",
      "train loss:0.3445843274686268\n",
      "train loss:0.4852569247841484\n",
      "train loss:0.38631928068780397\n",
      "train loss:0.41075769272287616\n",
      "train loss:0.4963320887267807\n",
      "train loss:0.3264248508711705\n",
      "train loss:0.4428280967301982\n",
      "train loss:0.4522156723543733\n",
      "train loss:0.3736346448008136\n",
      "train loss:0.48853070162019985\n",
      "train loss:0.4107401706661687\n",
      "train loss:0.4120000611518268\n",
      "train loss:0.40436962608758287\n",
      "train loss:0.3522275355610305\n",
      "train loss:0.2746709496538716\n",
      "train loss:0.32532241202240386\n",
      "train loss:0.21644195842705244\n",
      "train loss:0.4179456232146495\n",
      "train loss:0.2513482667489482\n",
      "train loss:0.364821705799177\n",
      "train loss:0.4745836887098404\n",
      "train loss:0.3776924168365094\n",
      "train loss:0.355130517049167\n",
      "train loss:0.3512029410058674\n",
      "train loss:0.40284667011721526\n",
      "train loss:0.5622751681684366\n",
      "=== epoch:3, train acc:0.89, test acc:0.876 ===\n",
      "train loss:0.41045750243029566\n",
      "train loss:0.36182024292853415\n",
      "train loss:0.35728855006937976\n",
      "train loss:0.513571634838243\n",
      "train loss:0.4872796933935723\n",
      "train loss:0.34843661033693435\n",
      "train loss:0.37311916538394485\n",
      "train loss:0.2533976723071099\n",
      "train loss:0.3620274877246036\n",
      "train loss:0.24841895590962526\n",
      "train loss:0.32945795200895367\n",
      "train loss:0.24948038080305718\n",
      "train loss:0.322660883134132\n",
      "train loss:0.20396256883908445\n",
      "train loss:0.3493859749378489\n",
      "train loss:0.38349671728035273\n",
      "train loss:0.323400578151972\n",
      "train loss:0.3762852849416221\n",
      "train loss:0.4227891716827368\n",
      "train loss:0.2288929768334408\n",
      "train loss:0.34231402950323697\n",
      "train loss:0.24012717214653798\n",
      "train loss:0.39112599418967003\n",
      "train loss:0.4351282413964085\n",
      "train loss:0.2204527958935186\n",
      "train loss:0.30562237123572866\n",
      "train loss:0.3061426426343088\n",
      "train loss:0.32018983803333717\n",
      "train loss:0.2328287814002431\n",
      "train loss:0.20532253361681466\n",
      "train loss:0.1881157374821862\n",
      "train loss:0.33633184857115717\n",
      "train loss:0.20002920546374342\n",
      "train loss:0.27362958790859726\n",
      "train loss:0.33809942760845885\n",
      "train loss:0.249557200742378\n",
      "train loss:0.3709288309767009\n",
      "train loss:0.21829559975853904\n",
      "train loss:0.32489130092754287\n",
      "train loss:0.2737142155648327\n",
      "train loss:0.29271306683466924\n",
      "train loss:0.20719327695921513\n",
      "train loss:0.3802185498196416\n",
      "train loss:0.33677773463467375\n",
      "train loss:0.3258530862005894\n",
      "train loss:0.30664342977422554\n",
      "train loss:0.2998506393959636\n",
      "train loss:0.4524857633040989\n",
      "train loss:0.30472633874359933\n",
      "train loss:0.21985061084300597\n",
      "=== epoch:4, train acc:0.891, test acc:0.887 ===\n",
      "train loss:0.23073254932736822\n",
      "train loss:0.26684998452157216\n",
      "train loss:0.28915064893438885\n",
      "train loss:0.26467207540313475\n",
      "train loss:0.2754705597097989\n",
      "train loss:0.25545089199388005\n",
      "train loss:0.31916517366051383\n",
      "train loss:0.28603712104586454\n",
      "train loss:0.19792947722453808\n",
      "train loss:0.35168074575790276\n",
      "train loss:0.3320569867570914\n",
      "train loss:0.22335512396951848\n",
      "train loss:0.25869671797726324\n",
      "train loss:0.20666800451924375\n",
      "train loss:0.19064670391317284\n",
      "train loss:0.2187977800260491\n",
      "train loss:0.27687680843461693\n",
      "train loss:0.2078899757670401\n",
      "train loss:0.38046220486075216\n",
      "train loss:0.3138710426601402\n",
      "train loss:0.22591100306175252\n",
      "train loss:0.16987648314326861\n",
      "train loss:0.21039772039424057\n",
      "train loss:0.1832637731097922\n",
      "train loss:0.31793766093802184\n",
      "train loss:0.1659302969529889\n",
      "train loss:0.15479043481192567\n",
      "train loss:0.2218771658584145\n",
      "train loss:0.25094223793927883\n",
      "train loss:0.18123120475117693\n",
      "train loss:0.18352956806477436\n",
      "train loss:0.3296084510273581\n",
      "train loss:0.09981968820405422\n",
      "train loss:0.2761529521704729\n",
      "train loss:0.2937431991493706\n",
      "train loss:0.12490366251255593\n",
      "train loss:0.10249988890048256\n",
      "train loss:0.18978460714588344\n",
      "train loss:0.23973556080806357\n",
      "train loss:0.18513941670984285\n",
      "train loss:0.1549854086440108\n",
      "train loss:0.2312033738042797\n",
      "train loss:0.3583769978396754\n",
      "train loss:0.24457372899352742\n",
      "train loss:0.34799167214456395\n",
      "train loss:0.34050868739711837\n",
      "train loss:0.3890156323923699\n",
      "train loss:0.2469370041012776\n",
      "train loss:0.12519914634587445\n",
      "train loss:0.1354583362934613\n",
      "=== epoch:5, train acc:0.909, test acc:0.898 ===\n",
      "train loss:0.18653856420665504\n",
      "train loss:0.3451330086761171\n",
      "train loss:0.17638610530876214\n",
      "train loss:0.30414865826255083\n",
      "train loss:0.4765423006352543\n",
      "train loss:0.2569393862591694\n",
      "train loss:0.11239056746662264\n",
      "train loss:0.2583415961167944\n",
      "train loss:0.23762022801000757\n",
      "train loss:0.2503544640002426\n",
      "train loss:0.25969490450069227\n",
      "train loss:0.2697069105904005\n",
      "train loss:0.31080556364348505\n",
      "train loss:0.20241486676314635\n",
      "train loss:0.09528773923291915\n",
      "train loss:0.14994698137909585\n",
      "train loss:0.26513563346566565\n",
      "train loss:0.22084487719127532\n",
      "train loss:0.15825370996253763\n",
      "train loss:0.2358454949453819\n",
      "train loss:0.2329501834473616\n",
      "train loss:0.1619408176784568\n",
      "train loss:0.32430568080687755\n",
      "train loss:0.17486874896273435\n",
      "train loss:0.12373941921484262\n",
      "train loss:0.14120964957734425\n",
      "train loss:0.1302070198810762\n",
      "train loss:0.14780133544905982\n",
      "train loss:0.209965806982367\n",
      "train loss:0.21775414332484064\n",
      "train loss:0.30510843878051735\n",
      "train loss:0.16621288319871103\n",
      "train loss:0.23718814061272833\n",
      "train loss:0.2297709144440121\n",
      "train loss:0.11937244167652518\n",
      "train loss:0.14239190383152298\n",
      "train loss:0.19161362371004212\n",
      "train loss:0.21528949333899675\n",
      "train loss:0.1578941581063414\n",
      "train loss:0.2550813164127101\n",
      "train loss:0.2085058423789908\n",
      "train loss:0.13989242732078477\n",
      "train loss:0.13197148451581864\n",
      "train loss:0.2229053084583808\n",
      "train loss:0.2041553023271822\n",
      "train loss:0.19641177090445985\n",
      "train loss:0.16925182838173358\n",
      "train loss:0.27536788077338203\n",
      "train loss:0.19686046757503853\n",
      "train loss:0.1690455814390332\n",
      "=== epoch:6, train acc:0.923, test acc:0.928 ===\n",
      "train loss:0.14950067143488335\n",
      "train loss:0.309143345172629\n",
      "train loss:0.16416057530454709\n",
      "train loss:0.18492237309849424\n",
      "train loss:0.19732091213821307\n",
      "train loss:0.153251510729273\n",
      "train loss:0.14830804722948948\n",
      "train loss:0.21918883478997375\n",
      "train loss:0.16153489698656784\n",
      "train loss:0.1664499780609594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.21355514814763102\n",
      "train loss:0.3307266806060604\n",
      "train loss:0.2938324262176735\n",
      "train loss:0.17042878044449408\n",
      "train loss:0.10897113138908188\n",
      "train loss:0.2086754803591505\n",
      "train loss:0.20080440880423908\n",
      "train loss:0.12834348730993814\n",
      "train loss:0.18544528890103582\n",
      "train loss:0.1345072857319182\n",
      "train loss:0.08472185262368313\n",
      "train loss:0.17062610196453964\n",
      "train loss:0.13440381289062578\n",
      "train loss:0.39377852789258855\n",
      "train loss:0.22210633613979744\n",
      "train loss:0.17251587690863587\n",
      "train loss:0.21950447444052226\n",
      "train loss:0.20366711979513663\n",
      "train loss:0.17765372579254254\n",
      "train loss:0.31454304002974526\n",
      "train loss:0.1516410772136992\n",
      "train loss:0.3271552145070222\n",
      "train loss:0.13757674019695407\n",
      "train loss:0.17614608631391332\n",
      "train loss:0.11517229095684327\n",
      "train loss:0.1788558216646258\n",
      "train loss:0.3452581726191234\n",
      "train loss:0.3599910079250577\n",
      "train loss:0.19964298588924403\n",
      "train loss:0.35562510862427016\n",
      "train loss:0.22948782040465646\n",
      "train loss:0.19735873483356264\n",
      "train loss:0.1079657682327577\n",
      "train loss:0.1788253805301817\n",
      "train loss:0.09467310307400145\n",
      "train loss:0.30868285470500395\n",
      "train loss:0.17828220274365883\n",
      "train loss:0.16169077911228896\n",
      "train loss:0.14640687058392043\n",
      "train loss:0.15867921758578052\n",
      "=== epoch:7, train acc:0.937, test acc:0.923 ===\n",
      "train loss:0.20396792124511273\n",
      "train loss:0.14099749700369796\n",
      "train loss:0.14362369965132518\n",
      "train loss:0.09992682112431844\n",
      "train loss:0.22176185608994156\n",
      "train loss:0.17291977491439223\n",
      "train loss:0.18723207246455623\n",
      "train loss:0.16293555820070366\n",
      "train loss:0.21201416398659387\n",
      "train loss:0.12898653252517286\n",
      "train loss:0.19002986624488397\n",
      "train loss:0.16305467981298022\n",
      "train loss:0.19047531212394975\n",
      "train loss:0.23640458682642296\n",
      "train loss:0.10391832210608656\n",
      "train loss:0.10737224853452719\n",
      "train loss:0.16205517986051027\n",
      "train loss:0.14969145967625438\n",
      "train loss:0.19408265507754133\n",
      "train loss:0.13826473462287756\n",
      "train loss:0.12654798518121432\n",
      "train loss:0.1328204984410924\n",
      "train loss:0.09440702236451429\n",
      "train loss:0.09809766561383919\n",
      "train loss:0.3181216264930755\n",
      "train loss:0.08153551057389916\n",
      "train loss:0.17714429876630242\n",
      "train loss:0.14525742581270598\n",
      "train loss:0.10805815860475139\n",
      "train loss:0.14109165274288937\n",
      "train loss:0.1329422707675668\n",
      "train loss:0.10387496168946767\n",
      "train loss:0.12162876208193925\n",
      "train loss:0.08312929484954791\n",
      "train loss:0.22346622317686662\n",
      "train loss:0.15760092145386034\n",
      "train loss:0.05315132494058406\n",
      "train loss:0.15331300820626695\n",
      "train loss:0.13592923544527483\n",
      "train loss:0.16498641311787832\n",
      "train loss:0.08743716678315673\n",
      "train loss:0.08886693386598626\n",
      "train loss:0.10507427884392742\n",
      "train loss:0.2026107706776835\n",
      "train loss:0.1377334438083749\n",
      "train loss:0.13243781290627088\n",
      "train loss:0.14195722473213673\n",
      "train loss:0.24572450335038212\n",
      "train loss:0.11877061730780403\n",
      "train loss:0.10918591038667375\n",
      "=== epoch:8, train acc:0.951, test acc:0.932 ===\n",
      "train loss:0.08576449749392104\n",
      "train loss:0.1303730987847912\n",
      "train loss:0.1091637839917409\n",
      "train loss:0.09840619539563536\n",
      "train loss:0.07418614805762208\n",
      "train loss:0.1492164501933483\n",
      "train loss:0.08574069641131173\n",
      "train loss:0.10564066955226123\n",
      "train loss:0.12027501604050947\n",
      "train loss:0.18011769363120847\n",
      "train loss:0.12112729924248165\n",
      "train loss:0.13561517331988657\n",
      "train loss:0.0764773056614865\n",
      "train loss:0.12306405838905059\n",
      "train loss:0.09840796058589295\n",
      "train loss:0.11044571275501389\n",
      "train loss:0.08736361638200624\n",
      "train loss:0.11678741549903282\n",
      "train loss:0.13284133103148793\n",
      "train loss:0.15067200542767292\n",
      "train loss:0.23140538499082922\n",
      "train loss:0.14849230468071525\n",
      "train loss:0.19205852901268639\n",
      "train loss:0.17660445137626632\n",
      "train loss:0.19241807986151496\n",
      "train loss:0.1317970500441165\n",
      "train loss:0.09814022653956485\n",
      "train loss:0.15085300396674467\n",
      "train loss:0.24918385838704893\n",
      "train loss:0.09312430987703499\n",
      "train loss:0.09084299283661888\n",
      "train loss:0.08143907095448336\n",
      "train loss:0.05378992144315328\n",
      "train loss:0.1990646699675668\n",
      "train loss:0.10137583819777105\n",
      "train loss:0.19840882277559682\n",
      "train loss:0.05817866231996887\n",
      "train loss:0.13816850930542263\n",
      "train loss:0.12609195660983089\n",
      "train loss:0.12379504601651302\n",
      "train loss:0.15422278115777233\n",
      "train loss:0.11140191982165502\n",
      "train loss:0.2340963519563614\n",
      "train loss:0.11691967095484536\n",
      "train loss:0.07006694599176087\n",
      "train loss:0.12116532985970599\n",
      "train loss:0.14201411921017132\n",
      "train loss:0.05484437149032675\n",
      "train loss:0.19322103653619963\n",
      "train loss:0.21965948365803756\n",
      "=== epoch:9, train acc:0.96, test acc:0.943 ===\n",
      "train loss:0.060519311709069\n",
      "train loss:0.094590649492036\n",
      "train loss:0.07091328728125881\n",
      "train loss:0.10352935211192837\n",
      "train loss:0.11467393428643825\n",
      "train loss:0.1374128269950551\n",
      "train loss:0.04752546302209459\n",
      "train loss:0.12397758736119395\n",
      "train loss:0.10720857094992856\n",
      "train loss:0.06909231287434095\n",
      "train loss:0.09253744652744066\n",
      "train loss:0.17930704965212255\n",
      "train loss:0.05304730456481828\n",
      "train loss:0.18906840243327314\n",
      "train loss:0.08498387111954869\n",
      "train loss:0.16016497484541112\n",
      "train loss:0.0668991947593603\n",
      "train loss:0.09855759029408981\n",
      "train loss:0.04949441260407309\n",
      "train loss:0.06398801180123112\n",
      "train loss:0.04015034145003191\n",
      "train loss:0.07759784004933516\n",
      "train loss:0.06845499122290355\n",
      "train loss:0.1302626030331523\n",
      "train loss:0.078106962993117\n",
      "train loss:0.04812212077209604\n",
      "train loss:0.06366078901726684\n",
      "train loss:0.08826927155218027\n",
      "train loss:0.1683099763398683\n",
      "train loss:0.16044683596730075\n",
      "train loss:0.14764725882471452\n",
      "train loss:0.09968746587585267\n",
      "train loss:0.08898015285138491\n",
      "train loss:0.04100960026292472\n",
      "train loss:0.1057655915068528\n",
      "train loss:0.09502925894040752\n",
      "train loss:0.1674877209140934\n",
      "train loss:0.08323342449100736\n",
      "train loss:0.1599970432466599\n",
      "train loss:0.05287454380388597\n",
      "train loss:0.1445997637891935\n",
      "train loss:0.13236874094717324\n",
      "train loss:0.21423614041139158\n",
      "train loss:0.07210411089370737\n",
      "train loss:0.07637202246820465\n",
      "train loss:0.025385843239465085\n",
      "train loss:0.14338253309547366\n",
      "train loss:0.12561891481130552\n",
      "train loss:0.09434381992560516\n",
      "train loss:0.044984315360708245\n",
      "=== epoch:10, train acc:0.961, test acc:0.948 ===\n",
      "train loss:0.12458675786979584\n",
      "train loss:0.10711806213217942\n",
      "train loss:0.11772983209248832\n",
      "train loss:0.06345925763352604\n",
      "train loss:0.08669860904713628\n",
      "train loss:0.0952675345131959\n",
      "train loss:0.0482987947063175\n",
      "train loss:0.02707577454118284\n",
      "train loss:0.049239170705246096\n",
      "train loss:0.048311644060868596\n",
      "train loss:0.09416417121676394\n",
      "train loss:0.06743900079605494\n",
      "train loss:0.06436343871736726\n",
      "train loss:0.17044062668511978\n",
      "train loss:0.03934928207960103\n",
      "train loss:0.12428356247843965\n",
      "train loss:0.1093318934773907\n",
      "train loss:0.04874788619220154\n",
      "train loss:0.08124671277539992\n",
      "train loss:0.05139648570502949\n",
      "train loss:0.06273145492141595\n",
      "train loss:0.1136198754461882\n",
      "train loss:0.09557260329850577\n",
      "train loss:0.0998240567405147\n",
      "train loss:0.09873748875153837\n",
      "train loss:0.14119789965638074\n",
      "train loss:0.04843253849178793\n",
      "train loss:0.04676204466062713\n",
      "train loss:0.08630990430596143\n",
      "train loss:0.07288226559429291\n",
      "train loss:0.029180858472695956\n",
      "train loss:0.04827402920581317\n",
      "train loss:0.08373252110145402\n",
      "train loss:0.09169933369391879\n",
      "train loss:0.1235855862515174\n",
      "train loss:0.1034430341764172\n",
      "train loss:0.1142449946073266\n",
      "train loss:0.085956967611685\n",
      "train loss:0.05537666444061113\n",
      "train loss:0.11550839655009044\n",
      "train loss:0.16828326877360922\n",
      "train loss:0.07233898557764967\n",
      "train loss:0.1362523606785049\n",
      "train loss:0.045650862118569925\n",
      "train loss:0.08149063287661162\n",
      "train loss:0.04548966986841697\n",
      "train loss:0.04969952133388789\n",
      "train loss:0.041295540989505754\n",
      "train loss:0.05371491215329106\n",
      "train loss:0.08019353595961175\n",
      "=== epoch:11, train acc:0.974, test acc:0.948 ===\n",
      "train loss:0.06716844697548818\n",
      "train loss:0.09819684385226982\n",
      "train loss:0.08568453600139751\n",
      "train loss:0.08007677437390281\n",
      "train loss:0.049909429352086934\n",
      "train loss:0.09892479023442345\n",
      "train loss:0.16807229772791865\n",
      "train loss:0.08886705281385814\n",
      "train loss:0.11183178233871013\n",
      "train loss:0.11465028785189679\n",
      "train loss:0.09564769815559744\n",
      "train loss:0.08759454363363048\n",
      "train loss:0.09560876725436845\n",
      "train loss:0.11765087847673701\n",
      "train loss:0.07062817311046643\n",
      "train loss:0.05067155058885115\n",
      "train loss:0.1026195479773075\n",
      "train loss:0.050217561475220295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03576280493193055\n",
      "train loss:0.06140122814635188\n",
      "train loss:0.09518790551124429\n",
      "train loss:0.0455752899841351\n",
      "train loss:0.08847690629834441\n",
      "train loss:0.09019834629630066\n",
      "train loss:0.09878859073424671\n",
      "train loss:0.04954740247521153\n",
      "train loss:0.12428309762507997\n",
      "train loss:0.0626156787526459\n",
      "train loss:0.13424526523452568\n",
      "train loss:0.04026483330905614\n",
      "train loss:0.061119365284935494\n",
      "train loss:0.03881055512187258\n",
      "train loss:0.027783419673796942\n",
      "train loss:0.07660528388855391\n",
      "train loss:0.04147333070766569\n",
      "train loss:0.03872532791976928\n",
      "train loss:0.043952601884516274\n",
      "train loss:0.05153344401024593\n",
      "train loss:0.038708278951500905\n",
      "train loss:0.06505921598358314\n",
      "train loss:0.07495197230727005\n",
      "train loss:0.06760105732315749\n",
      "train loss:0.06558511893393941\n",
      "train loss:0.03198242142299737\n",
      "train loss:0.06810326522955004\n",
      "train loss:0.05777144678763622\n",
      "train loss:0.054735052742272666\n",
      "train loss:0.08770001380675742\n",
      "train loss:0.08547215249844385\n",
      "train loss:0.0493485689487298\n",
      "=== epoch:12, train acc:0.975, test acc:0.945 ===\n",
      "train loss:0.0732704065473778\n",
      "train loss:0.18112124791571727\n",
      "train loss:0.07651269560564948\n",
      "train loss:0.11565288825750435\n",
      "train loss:0.06534268860039265\n",
      "train loss:0.038217470214140806\n",
      "train loss:0.03581960871484827\n",
      "train loss:0.09841155170363608\n",
      "train loss:0.03712027785666988\n",
      "train loss:0.08777475220945379\n",
      "train loss:0.2514373866209186\n",
      "train loss:0.04247856368896815\n",
      "train loss:0.05871809137749404\n",
      "train loss:0.061522931237160995\n",
      "train loss:0.030290344871525852\n",
      "train loss:0.06101541276694692\n",
      "train loss:0.05823285906100921\n",
      "train loss:0.05083642727742942\n",
      "train loss:0.14403685710265057\n",
      "train loss:0.043947189939324935\n",
      "train loss:0.03536775483064927\n",
      "train loss:0.04365802090422931\n",
      "train loss:0.047277336002967546\n",
      "train loss:0.08369271150273555\n",
      "train loss:0.09583198887030163\n",
      "train loss:0.05293174278299077\n",
      "train loss:0.0390104009181177\n",
      "train loss:0.07800262587589687\n",
      "train loss:0.07495796143300312\n",
      "train loss:0.13405819431880897\n",
      "train loss:0.07340350738874606\n",
      "train loss:0.04222858282808744\n",
      "train loss:0.07513521290616312\n",
      "train loss:0.0858998503657292\n",
      "train loss:0.0801280085234829\n",
      "train loss:0.1343640723816839\n",
      "train loss:0.09112469444025689\n",
      "train loss:0.02507601703106936\n",
      "train loss:0.08807668690632169\n",
      "train loss:0.055280847269976154\n",
      "train loss:0.048212107941620654\n",
      "train loss:0.06082070210353686\n",
      "train loss:0.11156436422247133\n",
      "train loss:0.07748126933886494\n",
      "train loss:0.05327905336781709\n",
      "train loss:0.10657697012357124\n",
      "train loss:0.12593889923194576\n",
      "train loss:0.04417586352773351\n",
      "train loss:0.06640575902855997\n",
      "train loss:0.032832434260600396\n",
      "=== epoch:13, train acc:0.979, test acc:0.952 ===\n",
      "train loss:0.03065299096285276\n",
      "train loss:0.02515380912601306\n",
      "train loss:0.0711691544654938\n",
      "train loss:0.03621585087968054\n",
      "train loss:0.0819861799704448\n",
      "train loss:0.04633143621981642\n",
      "train loss:0.03959608574492622\n",
      "train loss:0.033858285805382325\n",
      "train loss:0.07497721281224402\n",
      "train loss:0.11413174430329864\n",
      "train loss:0.05381618349909077\n",
      "train loss:0.07570425286422919\n",
      "train loss:0.04326889959515635\n",
      "train loss:0.027404253026716528\n",
      "train loss:0.03506448678516312\n",
      "train loss:0.028471942130895746\n",
      "train loss:0.09312816944003516\n",
      "train loss:0.055853054626750644\n",
      "train loss:0.053443087066929795\n",
      "train loss:0.05323267081829652\n",
      "train loss:0.04622515503751698\n",
      "train loss:0.035932424712892534\n",
      "train loss:0.07297459753728164\n",
      "train loss:0.09331059459800378\n",
      "train loss:0.07541146461892412\n",
      "train loss:0.030216219759357815\n",
      "train loss:0.03582751844924001\n",
      "train loss:0.052299091933678106\n",
      "train loss:0.052352376632582784\n",
      "train loss:0.03381588381319418\n",
      "train loss:0.08086707612568392\n",
      "train loss:0.04957640415227127\n",
      "train loss:0.030761587675931436\n",
      "train loss:0.060667554085890724\n",
      "train loss:0.10961347437691012\n",
      "train loss:0.0766420845760114\n",
      "train loss:0.04659073266582269\n",
      "train loss:0.03855536714924334\n",
      "train loss:0.04622254055881787\n",
      "train loss:0.08605035906710569\n",
      "train loss:0.060718498950769195\n",
      "train loss:0.052984523265347055\n",
      "train loss:0.033848917627300884\n",
      "train loss:0.060755282677938324\n",
      "train loss:0.03298774210143884\n",
      "train loss:0.07545545815388513\n",
      "train loss:0.12908309539166354\n",
      "train loss:0.08584354732056328\n",
      "train loss:0.09222290340187538\n",
      "train loss:0.0721667651449271\n",
      "=== epoch:14, train acc:0.979, test acc:0.953 ===\n",
      "train loss:0.07265317882607789\n",
      "train loss:0.08688296463207817\n",
      "train loss:0.03541155556622874\n",
      "train loss:0.10291148263387022\n",
      "train loss:0.10339757607984752\n",
      "train loss:0.07337458056030925\n",
      "train loss:0.062141830830381675\n",
      "train loss:0.044721290316967505\n",
      "train loss:0.1280217238767087\n",
      "train loss:0.05670646329866652\n",
      "train loss:0.049337405334653306\n",
      "train loss:0.08075973371749795\n",
      "train loss:0.02737873825526355\n",
      "train loss:0.05058322570389064\n",
      "train loss:0.03183697406277967\n",
      "train loss:0.034850472707479166\n",
      "train loss:0.06467448602580557\n",
      "train loss:0.03408496098685161\n",
      "train loss:0.05860435497044082\n",
      "train loss:0.040364565878975044\n",
      "train loss:0.05521924598396989\n",
      "train loss:0.11873020252191119\n",
      "train loss:0.027362234289541033\n",
      "train loss:0.08096564911212013\n",
      "train loss:0.029774098707372534\n",
      "train loss:0.06235921961277495\n",
      "train loss:0.029536549591703456\n",
      "train loss:0.028760884310004977\n",
      "train loss:0.05722015416817957\n",
      "train loss:0.050397982025853286\n",
      "train loss:0.11414746603891715\n",
      "train loss:0.017459835244041304\n",
      "train loss:0.0355840860170079\n",
      "train loss:0.028067834871023045\n",
      "train loss:0.09969310818775279\n",
      "train loss:0.055449035070070485\n",
      "train loss:0.06167362393187365\n",
      "train loss:0.0119020877131312\n",
      "train loss:0.056379897583187455\n",
      "train loss:0.036813578858279856\n",
      "train loss:0.06510013849973859\n",
      "train loss:0.04892557689291016\n",
      "train loss:0.061751418311203274\n",
      "train loss:0.0395001197125803\n",
      "train loss:0.026997018398126942\n",
      "train loss:0.042531422206130695\n",
      "train loss:0.052410476211884376\n",
      "train loss:0.050711251268295135\n",
      "train loss:0.08648338686656619\n",
      "train loss:0.06154681835046476\n",
      "=== epoch:15, train acc:0.984, test acc:0.957 ===\n",
      "train loss:0.0401245350029237\n",
      "train loss:0.03579537190104574\n",
      "train loss:0.02532678893915365\n",
      "train loss:0.022820374676815285\n",
      "train loss:0.055549216523187714\n",
      "train loss:0.028675737910658326\n",
      "train loss:0.0484173592497151\n",
      "train loss:0.026774981459247716\n",
      "train loss:0.04783341739562444\n",
      "train loss:0.03003199915235816\n",
      "train loss:0.04215810231855926\n",
      "train loss:0.03371462014054807\n",
      "train loss:0.021472914644705004\n",
      "train loss:0.03383266101334783\n",
      "train loss:0.01945683747397199\n",
      "train loss:0.04511104974351122\n",
      "train loss:0.05479101552884431\n",
      "train loss:0.03229041316964124\n",
      "train loss:0.04534214293975405\n",
      "train loss:0.026687241166674775\n",
      "train loss:0.030929684540824644\n",
      "train loss:0.022106551143364298\n",
      "train loss:0.11617211739790982\n",
      "train loss:0.043673917724492466\n",
      "train loss:0.06373955347736086\n",
      "train loss:0.01020176278421368\n",
      "train loss:0.014168400653069924\n",
      "train loss:0.011753801998014029\n",
      "train loss:0.028230118160034855\n",
      "train loss:0.04130805125390067\n",
      "train loss:0.053147252951654116\n",
      "train loss:0.033102567119884865\n",
      "train loss:0.03280505649418976\n",
      "train loss:0.05310379033460588\n",
      "train loss:0.01401108446329897\n",
      "train loss:0.022252871940762885\n",
      "train loss:0.0807559634942247\n",
      "train loss:0.036552612181564116\n",
      "train loss:0.029274784146568953\n",
      "train loss:0.0600374063117804\n",
      "train loss:0.09953530735236754\n",
      "train loss:0.07888127309763032\n",
      "train loss:0.03207583579428592\n",
      "train loss:0.03579147572157111\n",
      "train loss:0.06746692802082727\n",
      "train loss:0.021792781941999646\n",
      "train loss:0.03639584931496324\n",
      "train loss:0.040067553339509926\n",
      "train loss:0.0395928853018436\n",
      "train loss:0.03462782579554403\n",
      "=== epoch:16, train acc:0.985, test acc:0.961 ===\n",
      "train loss:0.012619904645427626\n",
      "train loss:0.013482370427953647\n",
      "train loss:0.048448696797695145\n",
      "train loss:0.03672185731705506\n",
      "train loss:0.04254231763332972\n",
      "train loss:0.056751745930336196\n",
      "train loss:0.04425348599452927\n",
      "train loss:0.04018309619738357\n",
      "train loss:0.030025609230331783\n",
      "train loss:0.06930506190615739\n",
      "train loss:0.009186218278418937\n",
      "train loss:0.08066359764331443\n",
      "train loss:0.026553690438777876\n",
      "train loss:0.06880420179403518\n",
      "train loss:0.0449461768491282\n",
      "train loss:0.047141252247139526\n",
      "train loss:0.02844848091565664\n",
      "train loss:0.03739572223600817\n",
      "train loss:0.032292959401766186\n",
      "train loss:0.025827831672816205\n",
      "train loss:0.03355547759343755\n",
      "train loss:0.016463826886578742\n",
      "train loss:0.0475620602599792\n",
      "train loss:0.05407853407034319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.04827708258340697\n",
      "train loss:0.0408587799524659\n",
      "train loss:0.028897280875387895\n",
      "train loss:0.013884001857858668\n",
      "train loss:0.0434846324998496\n",
      "train loss:0.022534470232244044\n",
      "train loss:0.021241701891579482\n",
      "train loss:0.042477083650822506\n",
      "train loss:0.01004703492071174\n",
      "train loss:0.044610805237126766\n",
      "train loss:0.021829761728657904\n",
      "train loss:0.014015924066673948\n",
      "train loss:0.06045256160008358\n",
      "train loss:0.03837976688738808\n",
      "train loss:0.05243695440303667\n",
      "train loss:0.03390296273993729\n",
      "train loss:0.01865715062646431\n",
      "train loss:0.02313068450508931\n",
      "train loss:0.02267588509328885\n",
      "train loss:0.02996127025364527\n",
      "train loss:0.03438433516861438\n",
      "train loss:0.01767653820116415\n",
      "train loss:0.015588694031898805\n",
      "train loss:0.04790692276994605\n",
      "train loss:0.0871358946594093\n",
      "train loss:0.03602876287349546\n",
      "=== epoch:17, train acc:0.99, test acc:0.96 ===\n",
      "train loss:0.017141852008956536\n",
      "train loss:0.0636434164290875\n",
      "train loss:0.023486580969226865\n",
      "train loss:0.030243311948677954\n",
      "train loss:0.03211595166356747\n",
      "train loss:0.04643459274953255\n",
      "train loss:0.052344726216709374\n",
      "train loss:0.01921137029901772\n",
      "train loss:0.031420248474356524\n",
      "train loss:0.07556978036122557\n",
      "train loss:0.03246195437729131\n",
      "train loss:0.0341379321871068\n",
      "train loss:0.02893173008954479\n",
      "train loss:0.03485758691226329\n",
      "train loss:0.02387553760048531\n",
      "train loss:0.03455799451510616\n",
      "train loss:0.015046773200440278\n",
      "train loss:0.026844136261379777\n",
      "train loss:0.021559838917511395\n",
      "train loss:0.019752562576495276\n",
      "train loss:0.016591028262963062\n",
      "train loss:0.028304675292293132\n",
      "train loss:0.054152669330668306\n",
      "train loss:0.03515807949613462\n",
      "train loss:0.026719040763392952\n",
      "train loss:0.04325689029308183\n",
      "train loss:0.02440626557709876\n",
      "train loss:0.07237166812525025\n",
      "train loss:0.026467924675607324\n",
      "train loss:0.03740576286571498\n",
      "train loss:0.03752946218478915\n",
      "train loss:0.03517209720046274\n",
      "train loss:0.019410285649237195\n",
      "train loss:0.04202965096978564\n",
      "train loss:0.11982952119136567\n",
      "train loss:0.0391100886859777\n",
      "train loss:0.015276012899505455\n",
      "train loss:0.01988878964313199\n",
      "train loss:0.025463207492573733\n",
      "train loss:0.030037634577701144\n",
      "train loss:0.03151441526970562\n",
      "train loss:0.033374159697539256\n",
      "train loss:0.016251597460327415\n",
      "train loss:0.01807599969228479\n",
      "train loss:0.024649732637610704\n",
      "train loss:0.02473824254995836\n",
      "train loss:0.025815021660648224\n",
      "train loss:0.040261705036856575\n",
      "train loss:0.01668131570127959\n",
      "train loss:0.020041156295477088\n",
      "=== epoch:18, train acc:0.988, test acc:0.962 ===\n",
      "train loss:0.03418992357294363\n",
      "train loss:0.035806435020419525\n",
      "train loss:0.06236679812588241\n",
      "train loss:0.032160339823892194\n",
      "train loss:0.027187471329834078\n",
      "train loss:0.017950155394393198\n",
      "train loss:0.012576606225072975\n",
      "train loss:0.028522437154288184\n",
      "train loss:0.01979343545718436\n",
      "train loss:0.013870286604704527\n",
      "train loss:0.013126944650436234\n",
      "train loss:0.009930168458838089\n",
      "train loss:0.03238167491902085\n",
      "train loss:0.020898740299195143\n",
      "train loss:0.03328943552422947\n",
      "train loss:0.03227576774362063\n",
      "train loss:0.025544826178145796\n",
      "train loss:0.10243342590413933\n",
      "train loss:0.014754507946693219\n",
      "train loss:0.03424975912594216\n",
      "train loss:0.027083884316589927\n",
      "train loss:0.04504886924583513\n",
      "train loss:0.03050236358475977\n",
      "train loss:0.022918740368846658\n",
      "train loss:0.013985664925934824\n",
      "train loss:0.02176899226050992\n",
      "train loss:0.018131959486486524\n",
      "train loss:0.015095831327188636\n",
      "train loss:0.100584961323287\n",
      "train loss:0.01649649298858781\n",
      "train loss:0.029422554771886614\n",
      "train loss:0.019349559346381683\n",
      "train loss:0.07935059652300132\n",
      "train loss:0.01651162590248775\n",
      "train loss:0.05388017467626602\n",
      "train loss:0.05613917767151629\n",
      "train loss:0.02445099622425089\n",
      "train loss:0.025401497512042914\n",
      "train loss:0.02113991812998085\n",
      "train loss:0.01778190347147136\n",
      "train loss:0.017603664574319576\n",
      "train loss:0.02440986148527391\n",
      "train loss:0.012408011551986115\n",
      "train loss:0.00927865382878895\n",
      "train loss:0.01639982522644\n",
      "train loss:0.03881742696109446\n",
      "train loss:0.025992704501806626\n",
      "train loss:0.019466375327493984\n",
      "train loss:0.022926149752443357\n",
      "train loss:0.013507212080708602\n",
      "=== epoch:19, train acc:0.986, test acc:0.96 ===\n",
      "train loss:0.01625645591685444\n",
      "train loss:0.01410823626955511\n",
      "train loss:0.03237740555365743\n",
      "train loss:0.03178154864044483\n",
      "train loss:0.018132505371996385\n",
      "train loss:0.019645927055132743\n",
      "train loss:0.01971365826337823\n",
      "train loss:0.07853766923233484\n",
      "train loss:0.013308973265222108\n",
      "train loss:0.051490891397575914\n",
      "train loss:0.007288857971542028\n",
      "train loss:0.025536015803391727\n",
      "train loss:0.0059175003460155175\n",
      "train loss:0.012474136542421297\n",
      "train loss:0.02098804937204411\n",
      "train loss:0.03939750946726811\n",
      "train loss:0.014100792458426787\n",
      "train loss:0.013319906250363098\n",
      "train loss:0.02599687257911979\n",
      "train loss:0.03977245196822674\n",
      "train loss:0.01329483837215728\n",
      "train loss:0.013667982232781001\n",
      "train loss:0.017872261329448344\n",
      "train loss:0.004801794956264394\n",
      "train loss:0.012974382220019318\n",
      "train loss:0.01024236190970084\n",
      "train loss:0.07385309051226568\n",
      "train loss:0.00992796598662593\n",
      "train loss:0.018461139334745084\n",
      "train loss:0.04225580581052117\n",
      "train loss:0.07572588127340162\n",
      "train loss:0.00935325639955593\n",
      "train loss:0.024837771383495917\n",
      "train loss:0.021468091102524105\n",
      "train loss:0.021986090912520975\n",
      "train loss:0.02148264913066663\n",
      "train loss:0.016192296664306095\n",
      "train loss:0.011580579521265207\n",
      "train loss:0.02137380037146778\n",
      "train loss:0.022196846898168556\n",
      "train loss:0.009961857519293911\n",
      "train loss:0.02808614856541977\n",
      "train loss:0.017811686414031976\n",
      "train loss:0.042311314356048596\n",
      "train loss:0.009853942473948645\n",
      "train loss:0.009782797287638454\n",
      "train loss:0.023349287125974293\n",
      "train loss:0.011669222779864867\n",
      "train loss:0.027171731321367597\n",
      "train loss:0.008067328249081452\n",
      "=== epoch:20, train acc:0.995, test acc:0.963 ===\n",
      "train loss:0.0034981788589568017\n",
      "train loss:0.017516431851104637\n",
      "train loss:0.012243540318145054\n",
      "train loss:0.009267728398995862\n",
      "train loss:0.013818832525880155\n",
      "train loss:0.012318722274012222\n",
      "train loss:0.0588203960535189\n",
      "train loss:0.014429196798257462\n",
      "train loss:0.02414850366058293\n",
      "train loss:0.04373710659521055\n",
      "train loss:0.008986093742804978\n",
      "train loss:0.01423827922705452\n",
      "train loss:0.01914563445074626\n",
      "train loss:0.044216381555649435\n",
      "train loss:0.009162541108907003\n",
      "train loss:0.010606894796755521\n",
      "train loss:0.01969356585663165\n",
      "train loss:0.021986187123682038\n",
      "train loss:0.021527844483006233\n",
      "train loss:0.04551319217082343\n",
      "train loss:0.012371065557487427\n",
      "train loss:0.005765652467565774\n",
      "train loss:0.022798052539625805\n",
      "train loss:0.022101240788406182\n",
      "train loss:0.030415113116384958\n",
      "train loss:0.020608153985362906\n",
      "train loss:0.025186877059758982\n",
      "train loss:0.010142893912441088\n",
      "train loss:0.02658845877265097\n",
      "train loss:0.06178109605653941\n",
      "train loss:0.017197150708256233\n",
      "train loss:0.00992708031746865\n",
      "train loss:0.032008710616117625\n",
      "train loss:0.13025395317767455\n",
      "train loss:0.023221146977247243\n",
      "train loss:0.010308954697109731\n",
      "train loss:0.033317657379829876\n",
      "train loss:0.02839189252646831\n",
      "train loss:0.014603625700902657\n",
      "train loss:0.006326383826677518\n",
      "train loss:0.030694467360332375\n",
      "train loss:0.01659543753110952\n",
      "train loss:0.013429324274471634\n",
      "train loss:0.019226528562276385\n",
      "train loss:0.019590292729320086\n",
      "train loss:0.03053705690245207\n",
      "train loss:0.02766752208945557\n",
      "train loss:0.06971681408902902\n",
      "train loss:0.013509739873096465\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.963\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 무작위(랜덤) 초기화 후의 가중치\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 학습된 가중치\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
