{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# 그래디언트 및 자동 미분 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6P32iYYV27b"
   },
   "source": [
    "## 자동 미분 및 그래디언트\n",
    "\n",
    "[자동 미분](https://en.wikipedia.org/wiki/Automatic_differentiation)은 신경망 학습을 위한 [역전파](https://en.wikipedia.org/wiki/Backpropagation)와 같은 머신러닝 알고리즘을 구현하는 데 유용합니다.\n",
    "\n",
    "이 가이드에서는 특히 [즉시 실행](eager.ipynb)에서 TensorFlow로 그래디언트를 계산하는 방법을 알아봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:04.395369Z",
     "iopub.status.busy": "2021-08-14T00:35:04.394833Z",
     "iopub.status.idle": "2021-08-14T00:35:06.307698Z",
     "shell.execute_reply": "2021-08-14T00:35:06.307194Z"
    },
    "id": "IqR2PQG4ZaZ0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## 그래디언트 계산하기\n",
    "\n",
    "자동으로 미분하기 위해 TensorFlow는 *정방향 패스* 동안 어떤 연산이 어떤 순서로 발생하는지 기억해야 합니다. 그런 다음 *역방향 패스* 동안 TensorFlow는 이 연산 목록을 역순으로 이동하여 그래디언트를 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CLWJl0QliB0"
   },
   "source": [
    "## 그래디언트 테이프\n",
    "\n",
    "텐서플로는 자동 미분(주어진 입력 변수에 대한 연산의 그래디언트(gradient)를 계산하는 것)을 위한 [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) API를 제공합니다. `tf.GradientTape`는 컨텍스트(context) 안에서 실행된 모든 연산을 테이프(tape)에 \"기록\"합니다. 그 다음 텐서플로는 [후진 방식 자동 미분(reverse mode differentiation)](https://en.wikipedia.org/wiki/Automatic_differentiation)을 사용해 테이프에 \"기록된\" 연산의 그래디언트를 계산합니다.\n",
    "\n",
    "예를 들면:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:06.312332Z",
     "iopub.status.busy": "2021-08-14T00:35:06.311766Z",
     "iopub.status.idle": "2021-08-14T00:35:07.868332Z",
     "shell.execute_reply": "2021-08-14T00:35:07.868679Z"
    },
    "id": "Xq9GgTCP7a4A"
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR9tFAP_7cra"
   },
   "source": [
    "일부 연산을 기록한 후에는 `GradientTape.gradient(target, sources)`를 사용하여 일부 소스(종종 모델 변수)에 상대적인 일부 대상(종종 손실)의 그래디언트를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:07.873501Z",
     "iopub.status.busy": "2021-08-14T00:35:07.872913Z",
     "iopub.status.idle": "2021-08-14T00:35:07.878793Z",
     "shell.execute_reply": "2021-08-14T00:35:07.879219Z"
    },
    "id": "LsvrwF6bHroC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2_aqsO25Vx1"
   },
   "source": [
    "위의 예제는 스칼라를 사용하지만, `tf.GradientTape`는 모든 텐서에서 쉽게 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:07.884851Z",
     "iopub.status.busy": "2021-08-14T00:35:07.884206Z",
     "iopub.status.idle": "2021-08-14T00:35:08.296859Z",
     "shell.execute_reply": "2021-08-14T00:35:08.296315Z"
    },
    "id": "vacZ3-Ws5VdV"
   },
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]  # (1,3)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x @ w + b  # (1,3)(3,2)+(2,)\n",
    "    loss = tf.reduce_mean(y**2)  #  1/2*(y^2)     (3,1)(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4eXOkrQ-9Pb"
   },
   "source": [
    "두 변수 모두에 대해 `loss`의 그래디언트를 가져오려면, 두 변수를 `gradient` 메서드에 소스로 전달할 수 있습니다. 테이프는 소스가 전달되는 방식에 대해 융통성이 있으며 목록 또는 사전의 중첩된 조합을 허용하고 같은 방식으로 구조화된 그래디언트를 반환합니다(`tf.nest` 참조)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.302002Z",
     "iopub.status.busy": "2021-08-14T00:35:08.301460Z",
     "iopub.status.idle": "2021-08-14T00:35:08.306154Z",
     "shell.execute_reply": "2021-08-14T00:35:08.306541Z"
    },
    "id": "luOtK1Da_BR0"
   },
   "outputs": [],
   "source": [
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ei4iVXi6qgM7"
   },
   "source": [
    "각 소스에 대한 그래디언트는 소스의 형상을 갖습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.311090Z",
     "iopub.status.busy": "2021-08-14T00:35:08.310494Z",
     "iopub.status.idle": "2021-08-14T00:35:08.313603Z",
     "shell.execute_reply": "2021-08-14T00:35:08.313139Z"
    },
    "id": "aYbWRFPZqk4U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)\n",
    "print(dl_dw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "print(b.shape)\n",
    "print(dl_db.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI_SzxHsvao1"
   },
   "source": [
    "다음은 그래디언트 계산입니다. 이번에는 변수의 사전을 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.318095Z",
     "iopub.status.busy": "2021-08-14T00:35:08.317498Z",
     "iopub.status.idle": "2021-08-14T00:35:08.322517Z",
     "shell.execute_reply": "2021-08-14T00:35:08.322891Z"
    },
    "id": "d73cY6NOuaMd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-3.2519531   0.31152344]\n",
      " [-6.5039062   0.6230469 ]\n",
      " [-9.755859    0.9345703 ]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor([-3.251404   0.3115234], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "my_vars = {\n",
    "    'w': w,\n",
    "    'b': b\n",
    "}\n",
    "\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "print(grad['w'])\n",
    "print(grad['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ2LvHifEMgO"
   },
   "source": [
    "## 모델에 대한 그래디언트\n",
    "\n",
    "[검사점 설정](checkpoint.ipynb) 및 [내보내기](saved_model.ipynb)를 위해 `tf.Variables`를 `tf.Module` 또는 해당 서브 클래스(<code>layers.Layer</code>와 <code>keras.Model</code>) 중 하나로 수집하는 것이 일반적입니다.\n",
    "\n",
    "대부분의 경우 모델의 훈련 가능한 변수에 대한 그래디언트를 계산하려고 합니다. `tf.Module`의 모든 서브 클래스는 `Module.trainable_variables` 속성에서 변수를 집계하므로 몇 줄의 코드로 이러한 그래디언트를 계산할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.327987Z",
     "iopub.status.busy": "2021-08-14T00:35:08.327411Z",
     "iopub.status.idle": "2021-08-14T00:35:08.475552Z",
     "shell.execute_reply": "2021-08-14T00:35:08.475948Z"
    },
    "id": "JvesHtbQESc-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_2/kernel:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[ 0.9089558 ,  0.9490924 ],\n",
      "       [ 0.36708176, -0.7026416 ],\n",
      "       [ 0.39744127,  0.8420503 ]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n",
      "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
      "array([[2.8359375, 2.0703125],\n",
      "       [5.671875 , 4.140625 ],\n",
      "       [8.5078125, 6.2109375]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([2.8359375, 2.0708005], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # Forward pass\n",
    "  y = layer(x)\n",
    "  loss = tf.reduce_mean(y**2)\n",
    "\n",
    "print(layer.trainable_variables)\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad = tape.gradient(loss, layer.trainable_variables)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.481055Z",
     "iopub.status.busy": "2021-08-14T00:35:08.480346Z",
     "iopub.status.idle": "2021-08-14T00:35:08.483297Z",
     "shell.execute_reply": "2021-08-14T00:35:08.482854Z"
    },
    "id": "PR_ezr6UFrpI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_2/kernel:0, shape: (3, 2)\n",
      "dense_2/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "  print(f'{var.name}, shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6Gx6LS714zR"
   },
   "source": [
    "<a id=\"watches\"></a>\n",
    "\n",
    "## 테이프의 감시 대상 제어하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4VlqKFzzGaC"
   },
   "source": [
    "기본 동작은 훈련 가능한 `tf.Variable`에 액세스한 후 모든 연산을 기록하는 것입니다. 그 이유는 다음과 같습니다.\n",
    "\n",
    "- 테이프는 역방향 패스의 그래디언트를 계산하기 위해 정방향 패스에 기록할 연산을 알아야 합니다.\n",
    "- 테이프는 중간 출력에 대한 참조를 보유하므로 불필요한 연산을 기록하지 않습니다.\n",
    "- 가장 일반적인 사용 사례는 모든 모델의 훈련 가능한 변수에 대해 손실의 그래디언트를 계산하는 것입니다.\n",
    "\n",
    "예를 들어, 다음은 `tf.Tensor`가 기본적으로 \"감시\"되지 않고 `tf.Variable`을 훈련할 수 없기 때문에 그래디언트를 계산하지 못합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.488975Z",
     "iopub.status.busy": "2021-08-14T00:35:08.488365Z",
     "iopub.status.idle": "2021-08-14T00:35:08.493573Z",
     "shell.execute_reply": "2021-08-14T00:35:08.493970Z"
    },
    "id": "Kj9gPckdB37a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "  print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkcpQnLgNxgi"
   },
   "source": [
    "`GradientTape.watched_variables` 메서드를 사용하여 테이프에서 감시 중인 변수를 나열할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.498367Z",
     "iopub.status.busy": "2021-08-14T00:35:08.497812Z",
     "iopub.status.idle": "2021-08-14T00:35:08.500777Z",
     "shell.execute_reply": "2021-08-14T00:35:08.501121Z"
    },
    "id": "hwNwjW1eAkib"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x0:0']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[var.name for var in tape.watched_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB9I1uFvB4tf"
   },
   "source": [
    "`tf.GradientTape`는 사용자가 감시 대상 또는 감시 예외 대상을 제어할 수 있는 후크를 제공합니다.\n",
    "\n",
    "`tf.Tensor`에 대한 그래디언트를 기록하려면 `GradientTape.watch(x)`를 호출해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.505730Z",
     "iopub.status.busy": "2021-08-14T00:35:08.505143Z",
     "iopub.status.idle": "2021-08-14T00:35:08.508029Z",
     "shell.execute_reply": "2021-08-14T00:35:08.508390Z"
    },
    "id": "tVN1QqFRDHBK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x**2\n",
    "\n",
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxsiYnf2DN8K"
   },
   "source": [
    "반대로, 모든 `tf.Variables`을 감시하는 기본 동작을 비활성화하려면, 그래디언트 테이프를 만들 때 `watch_accessed_variables=False`를 설정합니다. 이 계산은 두 가지 변수를 사용하지만, 변수 중 하나의 그래디언트만 연결합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.513169Z",
     "iopub.status.busy": "2021-08-14T00:35:08.512588Z",
     "iopub.status.idle": "2021-08-14T00:35:08.516833Z",
     "shell.execute_reply": "2021-08-14T00:35:08.516375Z"
    },
    "id": "7QPzwWvSEwIp"
   },
   "outputs": [],
   "source": [
    "x0 = tf.Variable(0.0)\n",
    "x1 = tf.Variable(10.0)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "  tape.watch(x1)\n",
    "  y0 = tf.math.sin(x0)\n",
    "  y1 = tf.nn.softplus(x1)\n",
    "  y = y0 + y1\n",
    "  ys = tf.reduce_sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRduLbE1H2IJ"
   },
   "source": [
    "`x0`에서 `GradientTape.watch`가 호출되지 않았으므로 이에 대한 그래디언트가 계산되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.520585Z",
     "iopub.status.busy": "2021-08-14T00:35:08.519102Z",
     "iopub.status.idle": "2021-08-14T00:35:08.523846Z",
     "shell.execute_reply": "2021-08-14T00:35:08.524231Z"
    },
    "id": "e6GM-3evH1Sz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx0: None\n",
      "dy/dx1: 0.9999546\n"
     ]
    }
   ],
   "source": [
    "# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n",
    "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
    "\n",
    "print('dy/dx0:', grad['x0'])\n",
    "print('dy/dx1:', grad['x1'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g1nKB6P-OnA"
   },
   "source": [
    "## 중간 결과\n",
    "\n",
    "`tf.GradientTape` 컨텍스트 내에서 계산된 중간값과 관련하여 출력의 그래디언트를 요청할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.528528Z",
     "iopub.status.busy": "2021-08-14T00:35:08.527965Z",
     "iopub.status.idle": "2021-08-14T00:35:08.530932Z",
     "shell.execute_reply": "2021-08-14T00:35:08.531301Z"
    },
    "id": "7XaPRAwUyYms"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "\n",
    "# Use the tape to compute the gradient of z with respect to the\n",
    "# intermediate value y.\n",
    "# dz_dy = 2 * y and y = x ** 2 = 9\n",
    "print(tape.gradient(z, y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISkXuY7YzIcS"
   },
   "source": [
    "기본적으로, `GradientTape.gradient` 메서드가 호출되면 `GradientTape`가 보유한 리소스가 해제됩니다. 동일한 계산에 대해 여러 그래디언트를 계산하려면 `persistent=True` 그래디언트 테이프를 만듭니다. 이렇게 하면 테이프 객체가 가비지 수집될 때 리소스가 해제되면 `gradient` 메서드를 여러 번 호출할 수 있습니다. 예를 들면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4. 108.]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A non-persistent GradientTape can only be used tocompute one set of gradients (or jacobians)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b20ec4a46898>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 108.0 (4 * x**3 at x = 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 6.0 (2 * x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     \"\"\"\n\u001b[0;32m   1026\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1027\u001b[1;33m       raise RuntimeError(\"A non-persistent GradientTape can only be used to\"\n\u001b[0m\u001b[0;32m   1028\u001b[0m                          \"compute one set of gradients (or jacobians)\")\n\u001b[0;32m   1029\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used tocompute one set of gradients (or jacobians)"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3.0])\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "\n",
    "print(tape.gradient(z, x).numpy())  # 108.0 (4 * x**3 at x = 3)\n",
    "print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.536324Z",
     "iopub.status.busy": "2021-08-14T00:35:08.535762Z",
     "iopub.status.idle": "2021-08-14T00:35:08.540184Z",
     "shell.execute_reply": "2021-08-14T00:35:08.539719Z"
    },
    "id": "zZaCm3-9zVCi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4. 108.]\n",
      "[2. 6.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y   # 2x*2x*x\n",
    "\n",
    "print(tape.gradient(z, x).numpy())  # 108.0 (4 * x**3 at x = 3)\n",
    "print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.543396Z",
     "iopub.status.busy": "2021-08-14T00:35:08.542844Z",
     "iopub.status.idle": "2021-08-14T00:35:08.545124Z",
     "shell.execute_reply": "2021-08-14T00:35:08.544651Z"
    },
    "id": "j8bv_jQFg6CN"
   },
   "outputs": [],
   "source": [
    "del tape   # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_ZY-9BUB7vX"
   },
   "source": [
    "## 성능에 대한 참고 사항\n",
    "\n",
    "- 그래디언트 테이프 컨텍스트 내에서 연산을 수행하는 것과 관련하여 작은 오버헤드가 있습니다. 대부분의 Eager 실행에는 상당한 비용이 들지 않지만, 필요한 경우에만 테이프 컨텍스트를 사용해야 합니다.\n",
    "\n",
    "- 그래디언트 테이프는 메모리를 사용하여 역방향 패스 동안 사용하기 위해 입력 및 출력을 포함한 중간 결과를 저장합니다.\n",
    "\n",
    "    효율성을 위해 (`ReLU`와 같은) 일부 연산은 중간 결과를 유지할 필요가 없으며 정방향 패스 동안에 정리됩니다. 그러나 테이프에서 `persistent=True`를 사용하면 *아무것도 삭제되지 않으며* 최대 메모리 사용량이 높아집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dLBpZsJebFq"
   },
   "source": [
    "## 스칼라가 아닌 대상의 그래디언트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pldU9F5duP2"
   },
   "source": [
    "그래디언트는 기본적으로 스칼라에 대한 연산입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.549506Z",
     "iopub.status.busy": "2021-08-14T00:35:08.548978Z",
     "iopub.status.idle": "2021-08-14T00:35:08.553701Z",
     "shell.execute_reply": "2021-08-14T00:35:08.553244Z"
    },
    "id": "qI0sDV_WeXBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "-0.25\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y0 = x**2\n",
    "    y1 = 1 / x  # x^-1 => -x^-2\n",
    "\n",
    "print(tape.gradient(y0, x).numpy())\n",
    "print(tape.gradient(y1, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COEyYp34fxj4"
   },
   "source": [
    "따라서, 여러 대상의 그래디언트를 요청하면 각 소스의 결과는 다음과 같습니다.\n",
    "\n",
    "- 대상의 합계 또는 그에 상응하는 그래디언트\n",
    "- 각 대상의 그래디언트의 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.558039Z",
     "iopub.status.busy": "2021-08-14T00:35:08.557522Z",
     "iopub.status.idle": "2021-08-14T00:35:08.561094Z",
     "shell.execute_reply": "2021-08-14T00:35:08.561466Z"
    },
    "id": "o4a6_YOcfWKS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.75\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  y0 = x**2\n",
    "  y1 = 1 / x\n",
    "\n",
    "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvP-mkBMgbym"
   },
   "source": [
    "마찬가지로, 대상이 스칼라가 아닌 경우 합계의 그래디언트가 계산됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.566112Z",
     "iopub.status.busy": "2021-08-14T00:35:08.565392Z",
     "iopub.status.idle": "2021-08-14T00:35:08.569378Z",
     "shell.execute_reply": "2021-08-14T00:35:08.568948Z"
    },
    "id": "DArPWqsSh5un"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x * [3., 4.]\n",
    "\n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flDbx68Zh5Lb"
   },
   "source": [
    "이렇게 하면, 손실 컬렉션 합계의 그래디언트 또는 요소별 손실 계산 합계의 그래디언트를 간단하게 구할 수 있습니다.\n",
    "\n",
    "각 항목에 대해 별도의 그래디언트가 필요한 경우, [야고비안](advanced_autodiff.ipynb#jacobians)을 참조하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwFswok8RAly"
   },
   "source": [
    "어떤 경우에는 야고비안을 건너뛸 수 있습니다. 요소별 계산의 경우, 각 요소가 독립적이므로 합의 그래디언트는 입력 요소와 관련하여 각 요소의 미분을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.574596Z",
     "iopub.status.busy": "2021-08-14T00:35:08.573571Z",
     "iopub.status.idle": "2021-08-14T00:35:08.581589Z",
     "shell.execute_reply": "2021-08-14T00:35:08.581943Z"
    },
    "id": "JQvk_jnMmTDS"
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(-10.0, 10.0, 200+1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = tf.nn.sigmoid(x)\n",
    "\n",
    "dy_dx = tape.gradient(y, x)   # y(1-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.604979Z",
     "iopub.status.busy": "2021-08-14T00:35:08.597630Z",
     "iopub.status.idle": "2021-08-14T00:35:08.800682Z",
     "shell.execute_reply": "2021-08-14T00:35:08.801117Z"
    },
    "id": "e_f2QgDPmcPE"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAq5UlEQVR4nO3deXxU9b3/8dcnk4SENRDCDrIIsskSEJeruCGLC4jautZWa12qv9rb9lbutdp6rV3t5q3VUqu2dUFbl2JFwK2itci+BQQBWUJI2CGQdWa+vz/OgEOcwAAzOZPJ+/l45DEz53xn5jNnknfOfOec79ecc4iISOOX4XcBIiKSGAp0EZE0oUAXEUkTCnQRkTShQBcRSROZfj1x+/btXc+ePf16ehGRRmnhwoU7nHMFsdb5Fug9e/ZkwYIFfj29iEijZGYb61unLhcRkTShQBcRSRMKdBGRNOFbH3ostbW1FBcXU1VV5XcpDS4nJ4du3bqRlZXldyki0kilVKAXFxfTqlUrevbsiZn5XU6Dcc6xc+dOiouL6dWrl9/liEgjddQuFzN70sy2mdmKetabmT1iZmvNbJmZFR5vMVVVVeTn5zepMAcwM/Lz85vkJxMRSZx4+tCfBsYfYf0EoG/k51bgsRMpqKmF+UFN9XWLSOIctcvFOTfHzHoeockk4M/OG4d3rpnlmVln59zWRBUpIunJOUd1MEx1bZiqYIiaYJhg2BEKh6kNOUJhRzDsCIbCkUtHMByOXHrXQ2FH2Dmcg7DzHtM5cLjIbbz13hMeahN24Pis/cE2B68DhMOfDS/uDqs76jqHD0F++LrYK0b2bMfofjHPDTohiehD7wpsjrpdHFn2uUA3s1vx9uLp0aNHAp5aRPzinGNfZZDt+6vYtq+anQdq2FdVy77KIOVVtXWuB6moCVFVG/3jhXhTmpLh4Afx28/tk7KBHquvIOZb5JybCkwFGDlyZBN6G0UaH+ccJXur2LjzAJt2VrBxVwWbdlVQsqeSbfuq2b6/mppgOOZ9MzOM1rlZtM7JpHVuFq1yMmnXIpucrAA5mRneZdbBy8Ch29mBDDIDRmZGBpkZRiDDyApkEMiwz5YHjMyMz64HMowMMzIMDMPMC84Ms88uAaKuH1xnddocvE50m6jXFd01evjyw1+/X12oiQj0YqB71O1uQEkCHrfB3XfffbRv3567774bgHvvvZeOHTvyjW98w+fKRJIrGAqzams5y7fsZdXWfXxcuo+Pt5ZTXh081CYzw+jWNpeubXMZ1asdBa2a0aFVMwoiP+1bNqNNbhatc7LIycrQ90I+SESgTwfuMrNpwOnA3kT0nz/wWhErS/adcHHRBnZpzfcvG1Tv+q9+9atcccUV3H333YTDYaZNm8a8efMSWoNIKgiGwizZvIePPt3FvE93sXDjbvZHwrtls0z6d2rFpOFdOKVTa3q3b0GPds3p3CaHzIDORUxlRw10M3seOA9ob2bFwPeBLADn3OPADOBiYC1QAdyUrGKTrWfPnuTn57N48WLKysoYPnw4+fn5fpclkhCVNSHe+Xgbb60q452Pt7G3shaAvh1aMmlYF0b1akdhj7Z0a5urvetGKp6jXK49ynoH3JmwiiKOtCedTLfccgtPP/00paWl3Hzzzb7UIJIozjnmfbqLlxYVM2N5Kfurg+Q1z+LCAR0YM6AjZ/TOp12LbL/LlARJqTNFU8HkyZO5//77qa2t5bnnnvO7HJHjUlUbYvqSEp74YD1ryvbTIjvAxad2ZnJhV0b1bKeukzSlQK8jOzub888/n7y8PAKBgN/liByTqtoQf/n3Rn4/Zx079tfQv1Mrfn7VEC4Z0pnm2fpzT3d6h+sIh8PMnTuXv/71r36XIhK3UNjx8qJifvXmGkr2VnH2ye2547w+nNWn6Q2l0ZQp0KOsXLmSSy+9lMmTJ9O3b1+/yxGJy6qt+5jy0jKWFu9lSLc2PPyFoZx1cnu/yxIfKNCjDBw4kPXr1/tdhkhcqmpDPPruWh775zra5Gbx66uHMWlYF+2RN2EKdJFGaOPOA3z92UUUlezjisKu3HfJQNrqaJUmT4Eu0sjMLirl239digFP3DiSMQM7+l2SpAgFukgj4Zzj1299wm/e/oRTu7bhd9cX0r1dc7/LkhSiQBdpBEJhx/deXcHz8zZx1Yhu/PDyweRk6bBaOZzOLjiKH/zgBzz88MNHbPP888/z0EMPfW55z5492bFjR7JKkyaiqjbEnc8u4vl5m/j6eX34+VVDFOYSkwI9AWbOnMn48Uea1Enk+NQEw9z2l4XMLCrlvksH8t3x/XUUi9RLgR7DQw89xCmnnMKYMWNYvXo1oVCIwsLPpkr95JNPGDFiBOD1ay5ZsoTCwkJ27tzJ2LFjGT58OLfddhsuMnL//PnzGTJkCFVVVRw4cIBBgwaxYkXMKVpFDgmFHf/54hLeW7Odn1xxKl89WxOIy5Glbh/6G1OgdHliH7PTqTDhJ0dssnDhQqZNm8bixYsJBoMUFhYyYsQI2rRpw5IlSxg2bBhPPfUUX/nKVwBYvHgxQ4cOxcx44IEHOPvss7n//vt5/fXXmTp1KgCnnXYaEydO5Hvf+x6VlZXccMMNDB48OLGvTdKKc477/r6C15dt5X8u7s81ozTDlxxd6ga6T95//30mT55M8+be0QMTJ04EvFEYn3rqKX75y1/ywgsvHBonfebMmUyYMAGAOXPm8PLLLwNwySWX0LZt20OPe//993PaaaeRk5PDI4880pAvSRqhX731Cc995PWZ3zq6j9/lSCORuoF+lD3pZIrVR3nllVfywAMPcMEFFzBixIhD46TPnj2bl1566Yj3Bdi1axf79++ntraWqqoqWrRokZzipdGbuaKUR97+hC+O7MZ/jTvF73KkEVEfeh2jR4/mlVdeobKykvLycl577TUAcnJyGDduHHfccQc33eTN4bF3716CweChcB89ejTPPvssAG+88Qa7d+8+9Li33norDz74INdffz333HNPA78qaSzWbivn2y8uYWj3PB68fLC+AJVjokCvo7CwkKuvvpphw4Zx5ZVXcs455xxad/3112NmjB07FoA333yTMWPGHFr//e9/nzlz5lBYWMjs2bPp0cPr9/zzn/9MZmYm1113HVOmTGH+/Pm88847DfvCJOWVV9Vy618Wkpsd4PEbCmmWqUMT5djYwSMxGtrIkSPdggULDlu2atUqBgwY4Es98Xj44YfZu3cvDz74IOD1q99yyy2cccYZCXn8VH/9kjzOOe56bjEzi0p59pbTOaO3pj6U2MxsoXNuZKx1qduHnmImT57MunXrDtuzfuKJJ3ysSNLJ9KUlvL58K98df4rCXI6bAj1Or7zyit8lSJoq21fFfa+uYHiPPG7TES1yAlKuD92vLiC/NdXX3dQ555jy0jJqQmF+8YWhBDL0Jagcv5QK9JycHHbu3Nnkws05x86dO8nJyfG7FGlgLy7YzLurtzNlfH96F7T0uxxp5FKqy6Vbt24UFxezfft2v0tpcDk5OXTr1s3vMqQB7dxfzUOvr+KM3u248cyefpcjaSClAj0rK4tevTRehTQND89eTUVNiB9ePpgMdbVIAqRUl4tIU7GseA/T5m/mK2f15OQOrfwuR9KEAl2kgYXDjh9MLyK/RTO+Maav3+VIGlGgizSwVxZvYdGmPdwz/hRa52T5XY6kEQW6SAOqqg3xs1kfM6x7HlcW6ktwSSwFukgDembuRsr2VTNlQn99ESoJp0AXaSAHqoM89s91nH1ye53eL0mhQBdpIE9/uIGdB2r41th+fpciaSquQDez8Wa22szWmtmUGOvbmNlrZrbUzIrM7KbElyrSeO2trOX3763jwv4dKOzR9uh3EDkORw10MwsAjwITgIHAtWY2sE6zO4GVzrmhwHnAL8wsO8G1ijRaf3x/PfuqgvznRdo7l+SJZw99FLDWObfeOVcDTAMm1WnjgFbmTa/SEtgFBBNaqUgjVV5Vy1P/2sCEwZ0Y3LWN3+VIGosn0LsCm6NuF0eWRfstMAAoAZYDdzvnwnUfyMxuNbMFZragKY7XIk3T8/M2UV4d5I7zNDSuJFc8gR7r2Kq6wyGOA5YAXYBhwG/NrPXn7uTcVOfcSOfcyIKCgmMsVaTxqQmGefKDDZzZO58h3fL8LkfSXDyBXgx0j7rdDW9PPNpNwMvOsxb4FOifmBJFGq/pS0so3VfFref29rsUaQLiCfT5QF8z6xX5ovMaYHqdNpuACwHMrCNwCrA+kYWKNDbOOf4wZz2ndGzFef30iVSS76iB7pwLAncBs4BVwIvOuSIzu93Mbo80exA4y8yWA28D9zjndiSraJHG4J9rtrO6rJyvje6Nd7yASHLFNR66c24GMKPOssejrpcAYxNbmkjj9oc56+nUOoeJQ7v4XYo0ETpTVCQJ1pSV8+G6ndx41klkZ+rPTBqGftNEkuDZuRvJDmRw9cjuR28skiAKdJEEO1Ad5OVFW5hwaifyWzbzuxxpQhToIgk2fWkJ5dVBvnTGSX6XIk2MAl0kgZxzPDN3I/07tWLESRqESxqWAl0kgZZs3kNRyT6uP+MkHaooDU6BLpJAz8zdRIvsAJOH1x3uSCT5FOgiCbKvqpZ/LCvh8uFdadksrlM8RBJKgS6SIK8v20p1MMzVp+lQRfGHAl0kQf62sJi+HVpyqsY8F58o0EUSYP32/SzcuJurRnTTl6HiGwW6SAK8tKiYDENfhoqvFOgiJygUdry8aAvn9iugQ+scv8uRJkyBLnKCPly3g617q7hqhL4MFX8p0EVO0N8WFtMmN4sLB3TwuxRp4hToIidgf3WQWUWlXDa0MzlZAb/LkSZOgS5yAt5cWUpVbZjLh+nLUPGfAl3kBExfUkLXvFwKe2ggLvGfAl3kOO0+UMP7n+zg0qGdycjQsefiPwW6yHGasWIrwbDTnKGSMhToIsdp+pIS+hS0YGDn1n6XIgIo0EWOS+neKuZt2MXEoV11qr+kDAW6yHH4x7ISnIOJw9TdIqlDgS5yHKYvLeHUrm3o1b6F36WIHKJAFzlGm3dVsKx4L5cN7ex3KSKHUaCLHKNZRaUAjB+kQJfUokAXOUazV5bRv1MreuQ397sUkcMo0EWOwc791SzYsIuxgzr5XYrI5yjQRY7B26u2EXYwblBHv0sR+RwFusgxmFVUSte8XJ1MJCkprkA3s/FmttrM1prZlHranGdmS8ysyMzeS2yZIv7bXx3k/bU7GDeok04mkpSUebQGZhYAHgUuAoqB+WY23Tm3MqpNHvA7YLxzbpOZaaR/STtz1mynJhhWd4ukrHj20EcBa51z651zNcA0YFKdNtcBLzvnNgE457YltkwR/80qKqVdi2xG9mzndykiMcUT6F2BzVG3iyPLovUD2prZP81soZndGOuBzOxWM1tgZgu2b99+fBWL+KAmGOadj7cxZkAHAhoqV1JUPIEe67fX1bmdCYwALgHGAfeZWb/P3cm5qc65kc65kQUFBcdcrIhf5q7fSXlVkLEDdbiipK6j9qHj7ZFHT2feDSiJ0WaHc+4AcMDM5gBDgTUJqVLEZ7OKSmmeHeDsvu39LkWkXvHsoc8H+ppZLzPLBq4Bptdp83fgHDPLNLPmwOnAqsSWKuKPcNjx5soyzu1XoImgJaUddQ/dORc0s7uAWUAAeNI5V2Rmt0fWP+6cW2VmM4FlQBh4wjm3IpmFizSUJcV72FZezTidHSopLp4uF5xzM4AZdZY9Xuf2z4GfJ640kdQwu6iMzAzj/P46GldSm84UFTkC5xyzi0o5s08+bXKz/C5H5IgU6CJHsG77ftbvOKDBuKRRUKCLHMGsojIALhqgs0Ml9SnQRY5gVlEpw7rn0alNjt+liByVAl2kHiV7KllWvJexGrtFGgkFukg93lzpdbfocEVpLBToIvWYvbKUkzu0pE9BS79LEYmLAl0khj0VNcxdv4uxA9XdIo2HAl0khrdXbSMUdupukUZFgS4Sw+yVpXRqncOpXdv4XYpI3BToInVU1oR4b812xg7qSIbGPpdGRIEuUsf7n2ynqjassc+l0VGgi9Qxq6iMNrlZnN5bU81J46JAF4kSDIV5++MyLuzfgayA/jykcdFvrEiUeRt2saeiVmeHSqOkQBeJMruojGaZGYzupzlvpfFRoItEHBz7/Jy+BTTPjmvuF5GUokAXiVixZR8le6sYp+4WaaQU6CIRs4pKyTC4UGOfSyOlQBeJmL2ylFG92tGuRbbfpYgcFwW6CPDpjgOsKduvsVukUVOgiwCzi0oBuEijK0ojpkAXwes/H9y1Nd3aNve7FJHjpkCXJm/bvioWb96jsVuk0VOgS5P35qoynNNUc9L4KdClyZtVVMZJ+c3p11FTzUnjpkCXJm1fVS3/XreDcYM6Yaaxz6VxU6BLk/bux9uoDTnNHSppQYEuTdrsojLat2xGYY+2fpcicsIU6NJkVdWGeHf1Nk01J2lDgS5N1ntrtlNRE2LCYB3dIukhrkA3s/FmttrM1prZlCO0O83MQmZ2VeJKFEmOmStKaZObxRm98/0uRSQhjhroZhYAHgUmAAOBa81sYD3tfgrMSnSRIolWEwzz1qoyLhrYUVPNSdqI5zd5FLDWObfeOVcDTAMmxWj3/4CXgG0JrE8kKf61bgflVUF1t0haiSfQuwKbo24XR5YdYmZdgcnA40d6IDO71cwWmNmC7du3H2utIgkzc3kpLZtlcnbf9n6XIpIw8QR6rK//XZ3bvwbucc6FjvRAzrmpzrmRzrmRBQWas1H8EQyFmb2ylAv6d6BZZsDvckQSJp6JE4uB7lG3uwElddqMBKZFzrRrD1xsZkHn3KuJKFIkkeZ9uovdFbXqbpG0E0+gzwf6mlkvYAtwDXBddAPnXK+D183saeAfCnNJVW+sKCUnK4NzT9GnREkvRw1051zQzO7CO3olADzpnCsys9sj64/Yby6SSsJhx6yiUs7r14Hm2fHsz4g0HnH9RjvnZgAz6iyLGeTOua+ceFkiybFo0262lVcz4VR1t0j60QG40qS8saKU7EAGF/Tv4HcpIgmnQJcmwznHzBWlnN23Pa1ysvwuRyThFOjSZCzfspcteyoZr6NbJE0p0KXJeG1pCVkB09jnkrYU6NIkhMOOfyzbyui+BeQ1z/a7HJGkUKBLkzB/wy627q1i4rAufpcikjQKdGkSpi8tIScrgzED1N0i6UuBLmmvNhRmxvKtjBnQkRbNdDKRpC8FuqS9D9buYHdFLROHqrtF0psCXdLea0tLaJWTqbFbJO0p0CWtVdWGmF1UxvhBnTRUrqQ9BbqktbdXbWN/dVBHt0iToECXtPbSomI6tm7GWX00M5GkPwW6pK1t+6p4b812rijsRiAj1sRbIulFgS5p69UlWwiFHVcWdvO7FJEGoUCXtOSc46WFWxjeI4+TO7T0uxyRBqFAl7S0Yss+VpeVc9UI7Z1L06FAl7T0t4Wbyc7M4NIhOrpFmg4FuqSd6mCIvy8tYdygTrTJ1UQW0nQo0CXtvLVyG3sqatXdIk2OAl3SzrMfbaRrXi5nn6xjz6VpUaBLWlm3fT8frtvJdaf30LHn0uRoLFFJK8/O3URWwPjiyO7HfufaSti9Ear2QHZLyOsOOW0SXqNIsijQJW1U1oT428LNjBvUiYJWzeK7U/V+WPE3WPoCFM+HcG3USoOOg2HQ5TD8S9BKk2NIalOgS9p4bWkJ+6qCfOmMk47eOBSEBU/Cez+Fih1QMADO/Dp0GgK5eVBzALavhnXvwjsPwpyfw+m3wznfhpzWSX8tIsdDgS5p45mPNtKvY0tG9Wp35IY71sKrt3t75D3PgQu+B91PB4vR537ud732c34O//oNrHgZLv8d9DonOS9C5AToS1FJC8uK97CseC/Xn34SFiuYD/p4Bkw9D3Z8Alf+Eb78GvQ4I3aYH9T+ZLji93DzLAhkwp8neuHuXMJfh8iJUKBLWvjjB5/SIjvA5MKu9Tea+xhMuxby+8Ad/4JTrzpykNfV43S47X0YMBHevB9euxvC4RMvXiRBFOjS6BXvruAfy7ZyzagetM6p58zQd38EM6fAgMvg5pnQ5jhPOmrWEr7wtNeXvuhP8OodXn+8SApQH7o0ek9+sAGAm8/uFbvBnIe9Lz+H3wCXPQIZJzgVnRlceD9k5sK7P4RQNVzxBwhomAHxV1x76GY23sxWm9laM5sSY/31ZrYs8vOhmQ1NfKkin7e3opZp8zcxcWgXuublfr7B3Me9o1SGXA2X/d+Jh3m0c/8Lxv4Qil6BV25T94v47qh76GYWAB4FLgKKgflmNt05tzKq2afAuc653WY2AZgKnJ6MgkWiPfPRRipqQnztnN6fX7n4GZh5D/S/FCb9DjKS0MN41v+DcBDe+gG06gzjHkr8c4jEKZ4ul1HAWufcegAzmwZMAg4FunPuw6j2cwGNiiRJVx0M8fSHGzinb3sGdqlzbPinc7wvLXufD1c96R2dkiz/8U3YVwL//i207gJn3pm85xI5gnh2WboCm6NuF0eW1eerwBuxVpjZrWa2wMwWbN++Pf4qRWJ4edEWtpdXc9voPoev2PUpvPhlaNcHvvhnyIzzrNHjZQbjf+J94Trrf7wuGBEfxBPosY7rinkArpmdjxfo98Ra75yb6pwb6ZwbWVBQEH+VInVUB0P89p21DO2ex3+cnB+1ohymXQcuDNc+33BndWYEvC9Gu58Or9wBW5c2zPOKRIkn0IuB6JGOugEldRuZ2RDgCWCSc25nYsoTie2F+ZvZsqeS74zt99mJROEwvHybd8r+F//kHW/ekLJy4epnoHk7eP462K9PodKw4gn0+UBfM+tlZtnANcD06AZm1gN4GfiSc25N4ssU+UxlTYj/e2cto3q1O3zM83/+CFa/DuN/DL3P86e4lh3gmme98WFevBGCNf7UIU3SUQPdORcE7gJmAauAF51zRWZ2u5ndHml2P5AP/M7MlpjZgqRVLE3eM3M3sr28mu+MPeWzvfMVL3njrQz/Eoy61d8CuwyHSY/Cpg/hje/6W4s0KXF99e+cmwHMqLPs8ajrtwC3JLY0kc/bXx3ksffWMbpfwWeDcJUsgVfvhO5nwCW/OLbT+ZPl1KugdDn869fQaTCcpj8PST6d+i+Nyh/mrGfXgRq+fVE/b8H+bTDtemieD1f/JflHtByLC++HvuPgjXtgwwd+VyNNgAJdGo3Nuyp4/L11XDKkM0O750GwGl64ASp2wrXPef3XqSQjAFf+Adr19vrTd2/0uyJJcwp0aTR+NGMVGWbce/EAb+ja178Fmz+CyY9B5xQdbSKnDVzzvDeA17TrvBmSRJJEgS6Nwr/W7uCNFaXceX4fuuTlwke/907tH/1dGDTZ7/KOrP3J8IUnYdtKb3RGjfkiSaJAl5RXGwrz/elF9GjXnFvO6Q2fvAWz/tsbo+W8//a7vPicPAYu+l9YNd07GkckCTR8rqS8p/71KWu37eeJG0eSs/sT+NtN0GEQTP59cgbcSpYz74KyIu94+Y4DvaECRBKoEf01SFO0dls5D89ew5gBHbmwRwY890XvjMzrpnmTTTQmZnDpr6HrSO+M1rIivyuSNKNAl5QVDIX59otLaZEd4EcT+2Iv3AD7y7wvGY93xiG/ZeV4wwM0awXPXwMHdvhdkaQRBbqkrMffW8fS4r38cNJgOrz7Xdg8Fy5/DLqN8Lu0E9O6M1zznHcM/XNXQ02F3xVJmlCgS0oqKtnLb97+hMuGduGSPc/Asmlw/r0w+Aq/S0uMbiPgyj9CySJ46aual1QSQoEuKWdfVS13PbeYvObZ/KTHfHj3IRh6LYz+L79LS6wBl8KEn8HqGd6YLy7mqNQicdNRLpJSwmHHt15YwuZdFcy8aAct3vyud/r8xP9LjTFaEm3U12BvsTfmS/N8uOBevyuSRkyBLinlt++u5a1V25h61h5Ofv9b0OMM+MLTEMjyu7TkufD73vAFc37mfWl6zrf9rkgaKQW6pIx3Pi7jV2+tYUrfLVy09HtQ0B+unQbZzf0uLbkyMuCy30CwCt7+X8jMhTO/7ndV0ggp0CUlLNy4mzufXcyX8tdwW8mPsYJ+cON0yM3zu7SGkRGAyx/3BhybFTn7VaEux0hfiorvVm3dx01PzWNy86U8UPkjrEN/L8ybt/O7tIYVyPSOfBlwmRfq7/5IX5TKMVGgi6827jzAl/44j+sDb/NQzU+wjoPhxr83vTA/KDMbrnoaht0A7/3UG0tdg3lJnNTlIr5ZU1bOjU/8m68Hn+VmXoW+Y+GqpxrfKf2JFsj0jurJaQNzH4XyEm/cmuwWflcmKU576OKLhRt3ceNj7/Bg8FdemI/4indKf1MP84MyMmDcQzD2Ifj4dfjjONizye+qJMUp0KXBvbWyjPueeJlp9j+MYS6MecAbtCqgD4yHMYOz7oLrXoQ9G2Hq+fDpHL+rkhSmQJcGEwo7Hp75Ma8982teCtxLj5wq7EuvwtnfTM+ThhKl70Vwy9uQ2xb+NBHeegBCtX5XJSlIgS4NYsf+au6cOotB/7qL32T/jmbdhpBxx/vQ+1y/S2scCvrBbe/B8Bvgg1/Ck+Ngx1q/q5IUo8+4klTOOV5bWsJHf/89Pwo/SZusKrjgATLOvEtdLMcquwVM+i2cfCG8djc8dqZ3VunZ/wmZzfyuTlKA/qIkaUr2VPLECy8xYctveChjDZUdhhL4wlTo0N/v0hq3QZOhx1kw63/gnz+G5X+FcT/2umbUddWkKdAl4fZW1DJt5rt0XPII37MPqM5pS3jcI+QOv8E7I1JOXKuOcNUfYdh1MOM78NwXvJAf8wPocbrf1YlPzPl0JtrIkSPdggULfHluSY69FbXMeOdt2ix4hHHuQ0IZ2VQX3kKri6ZATmu/y0tfwRpY/Gd472fejE69z/eOjulzofbY05CZLXTOjYy5ToEuJ2pD2R7mzfwLJ61/jtNtJVWWw/4hN9H+om9DywK/y2s6ag7AvKkw93HYXwodBnrD8w6+0jtJSdKCAl0Sbl9lNfPem0lw2V8ZceA9CmwfO7M6ERx+Mx3P+1rTPXU/FQRrYMXf4N+PQtkKyMyBARNhyBeh12h9gdrIHSnQ1YcucSvZvoPV/34dt2YWA8r/zRjbRTXZbC44h8yzbyJ/yMXqI08Fmdle3/rQa6FkMSx+xgv45S9Cs9beEAv9xkHPc7z5TSVtaA9dYnLOsXnzRjYveYfaDf+mw57F9A2tJ8tCVJDDprzTyRkyiZPOugrTx/nUF6yG9e/BqunelHcVO73l+X2h1znQ82zoUghte6rfPcWpy0Xq5Zxj9+5dlG1cxZ4NSwltLaL53jV0qV5PJ7w/+mqy2JTTn4pOp9H+1IvoMuQCLCvH58rluIVDULocNrwPn74PGz+EmnJvXbM20OlU6DwEOg6Cdn0gvw+0KFDQp4gTDnQzGw/8BggATzjnflJnvUXWXwxUAF9xzi060mMq0JMvHAqzd88O9pRtpnznFqp2lRDcVwr7y8g6sJXWlcUUBEtpZ+WH7lPjMtmS1YM9rfriOgyiw6Bz6TrgDAV4OgsFoWw5bF0KW5d5l2VFEKz8rE2z1tCuN7TpBq06QavO3k/ryGXzfMjJ87p7JKlOqA/dzALAo8BFQDEw38ymO+dWRjWbAPSN/JwOPBa5FCAcChEKBQkFawkGawkFg4SCNYTDIUKR2+FgLaFQkHColnCwltqaSoLVFYRqqgjXVBCqriRcW4mrrcJFLglWYbUVBGrKyawtJztYTrPQAZqH99PC7aeFq6SthWlbp55ql8XOjHx2NevCuryBrG3bk2YFfSjoPYROPQfRK0t/lE1KIBO6DPd+DgoFvQHBdq2Hnetg17rI5XrY8AFU7Yn9WFktvFmmcvK8y9y23j+D7OaQlQtZBy9bRC4PLsuBjCxv7thA1mfXM7K8+g7dzvxseUYALCPyo08PEN+XoqOAtc659QBmNg2YBEQH+iTgz87b3Z9rZnlm1tk5tzXRBS9792+0ef/7ABgOi/qEYTjARS7Be4s/a3PodlSbQ49Tz+3o+3y27vOPG+sxAoQJECJgjgwg0dMc17oAVZbNAVpQmdGSqkALDmQXsCerN+FmrQk3a43ltiMrrzM5bbvQuqArbTt0p0XrdnQxo0uC65E0Esj0ulry+3hnoNZVU+EdGrlvK5RvhcrdULnHC/rKPd7tqj2w61Oo3ge1FVBb6V0mi2UAFhXydX8s8hNjHRb1TyHqn4PVuXLYP466y6LvZ/W0iSwrvNE7VyDB4gn0rsDmqNvFfH7vO1abrsBhgW5mtwK3AvTo0eNYawUgu2UeO5v3ORSr3oayOrfhUBRH1kdv2JhtP7c8+s3J+Kxd9GPY4Y9b9z4uI+DtUWRkensTGVlYZNmhy4C3PiOQCYFMMiLtA9k5ZGbnEmiWS1azFmTn5JLdrDnZuS3IzmlOTm4LsjKzyAJaHdeWFDkB2c29Lph2vY/tfuGwNxn2wXCPvgzVQLjW+3QQrvVGlAwHI5cxbruwN0WfC9f5qbuszm3quQ/UmfKv7rKodXWXHev9WnY4tu0Wp3gCPdZnmbod7/G0wTk3FZgKXh96HM/9Of1PGwOnjTmeu4qI3zIyvH8G2c2BfL+rSTvxDJ9bDHSPut0NKDmONiIikkTxBPp8oK+Z9TKzbOAaYHqdNtOBG81zBrA3Gf3nIiJSv6N2uTjngmZ2FzAL77DFJ51zRWZ2e2T948AMvEMW1+IdtnhT8koWEZFY4jr13zk3Ay+0o5c9HnXdAXcmtjQRETkWmoJORCRNKNBFRNKEAl1EJE0o0EVE0oRvoy2a2XZg43HevT2wI4HlJEqq1gWpW5vqOjaq69ikY10nOediTgXmW6CfCDNbUN9oY35K1bogdWtTXcdGdR2bplaXulxERNKEAl1EJE001kCf6ncB9UjVuiB1a1Ndx0Z1HZsmVVej7EMXEZHPa6x76CIiUocCXUQkTaRsoJvZF8ysyMzCZjayzrr/NrO1ZrbazMbVc/92ZvammX0Suaw7tWYianzBzJZEfjaY2ZJ62m0ws+WRdkmfGdvMfmBmW6Jqu7ieduMj23CtmU1pgLp+bmYfm9kyM3vFzPLqadcg2+torz8yHPQjkfXLzKwwWbVEPWd3M3vXzFZFfv/vjtHmPDPbG/X+3p/suqKe+4jvjU/b7JSobbHEzPaZ2TfrtGmQbWZmT5rZNjNbEbUsrixKyN+jcy4lf4ABwCnAP4GRUcsHAkuBZkAvYB0QiHH/nwFTItenAD9Ncr2/AO6vZ90GoH0DbrsfAN85SptAZNv1BrIj23RgkusaC2RGrv+0vvekIbZXPK8fb0joN/Bm5DoD+KgB3rvOQGHkeitgTYy6zgP+0VC/T8fy3vixzWK8r6V4J980+DYDRgOFwIqoZUfNokT9PabsHrpzbpVzbnWMVZOAac65aufcp3hjsI+qp92fItf/BFyelELx9kqALwLPJ+s5kuDQ5N/OuRrg4OTfSeOcm+2cC0ZuzsWb2cov8bz+Q5OfO+fmAnlm1jmZRTnntjrnFkWulwOr8ObnbSwafJvVcSGwzjl3vGehnxDn3BxgV53F8WRRQv4eUzbQj6C+Canr6ugisyZFLpMzK6vnHKDMOfdJPesdMNvMFkYmym4Id0U+8j5Zz0e8eLdjstyMtycXS0Nsr3hev6/byMx6AsOBj2KsPtPMlprZG2Y2qKFq4ujvjd+/V9dQ/46VX9ssnixKyHaLa4KLZDGzt4BOMVbd65z7e313i7EsacdexlnjtRx57/w/nHMlZtYBeNPMPo78J09KXcBjwIN42+VBvO6gm+s+RIz7nvB2jGd7mdm9QBB4tp6HSfj2ilVqjGXHNfl5MphZS+Al4JvOuX11Vi/C61LYH/l+5FWgb0PUxdHfGz+3WTYwEfjvGKv93GbxSMh28zXQnXNjjuNu8U5IXWZmnZ1zWyMf+bYlo0YzywSuAEYc4TFKIpfbzOwVvI9XJxRQ8W47M/sD8I8Yq5IysXcc2+vLwKXAhS7SeRjjMRK+vWJI2cnPzSwLL8yfdc69XHd9dMA752aY2e/MrL1zLumDUMXx3vg5YfwEYJFzrqzuCj+3GfFlUUK2W2PscpkOXGNmzcysF95/2Xn1tPty5PqXgfr2+E/UGOBj51xxrJVm1sLMWh28jvfF4IpYbROlTp/l5HqeL57JvxNd13jgHmCic66injYNtb1ScvLzyPcxfwRWOed+WU+bTpF2mNkovL/jncmsK/Jc8bw3fk4YX+8nZb+2WUQ8WZSYv8dkf+t7vD94QVQMVANlwKyodffifSO8GpgQtfwJIkfEAPnA28Ankct2SarzaeD2Osu6ADMi13vjfWO9FCjC63pI9rb7C7AcWBb5pehct67I7YvxjqJY10B1rcXrJ1wS+Xncz+0V6/UDtx98P/E+Bj8aWb+cqKOtkljT2XgftZdFbaeL69R1V2TbLMX7cvmsZNd1pPfG720Wed7meAHdJmpZg28zvH8oW4HaSH59tb4sSsbfo079FxFJE42xy0VERGJQoIuIpAkFuohImlCgi4ikCQW6iEiaUKCLiKQJBbqISJpQoItEmNlpkQHNciJnRRaZ2WC/6xKJl04sEoliZj8EcoBcoNg592OfSxKJmwJdJEpkHI35QBXe6eEhn0sSiZu6XEQO1w5oiTdbUI7PtYgcE+2hi0Qxs+l4s8X0whvU7C6fSxKJm6/joYukEjO7EQg6554zswDwoZld4Jx7x+/aROKhPXQRkTShPnQRkTShQBcRSRMKdBGRNKFAFxFJEwp0EZE0oUAXEUkTCnQRkTTx/wGw/D/M69kLuAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y, label='y')\n",
    "plt.plot(x, dy_dx, label='dy/dx')\n",
    "plt.legend()\n",
    "_ = plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kADybtQzYj4"
   },
   "source": [
    "## 흐름 제어하기\n",
    "\n",
    "그래디언트 테이프는 실행되는 연산을 기록하기 때문에 Python 제어 흐름이 자연스럽게 처리됩니다(예: `if` 및 `while` 구문).\n",
    "\n",
    "여기서는 `if`의 각 분기에 서로 다른 변수가 사용됩니다. 그래디언트는 사용된 변수에만 연결됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.808363Z",
     "iopub.status.busy": "2021-08-14T00:35:08.807367Z",
     "iopub.status.idle": "2021-08-14T00:35:08.812478Z",
     "shell.execute_reply": "2021-08-14T00:35:08.812876Z"
    },
    "id": "ciFLizhrrjy7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(-1.0)\n",
    "\n",
    "v0 = tf.Variable(2.0)\n",
    "v1 = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  if x > 0.0:\n",
    "    result = v0\n",
    "  else:\n",
    "    result = v1**2 \n",
    "\n",
    "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
    "\n",
    "print(dv0)\n",
    "print(dv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKnLaiapsjeP"
   },
   "source": [
    "제어문 자체는 미분할 수 없으므로 그래디언트 기반 최적화 프로그램에는 보이지 않습니다.\n",
    "\n",
    "위 예제에서 `x` 값에 따라 테이프는 `result = v0` 또는 `result = v1**2`를 기록합니다. `x`에 대한 그래디언트는 항상 `None`입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.817475Z",
     "iopub.status.busy": "2021-08-14T00:35:08.816777Z",
     "iopub.status.idle": "2021-08-14T00:35:08.820020Z",
     "shell.execute_reply": "2021-08-14T00:35:08.819420Z"
    },
    "id": "8k05WmuAwPm7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "dx = tape.gradient(result, x)\n",
    "\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egypBxISAHhx"
   },
   "source": [
    "## `None`의 그래디언트 구하기\n",
    "\n",
    "대상이 소스에 연결되어 있지 않으면 그래디언트는 `None`입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.825307Z",
     "iopub.status.busy": "2021-08-14T00:35:08.824262Z",
     "iopub.status.idle": "2021-08-14T00:35:08.828326Z",
     "shell.execute_reply": "2021-08-14T00:35:08.828735Z"
    },
    "id": "CU185WDM81Ut"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.)\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = y * y\n",
    "print(tape.gradient(z, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZbKpHfBRJym"
   },
   "source": [
    "여기서 `z`는 명확하게 `x`에 연결되어 있지 않지만, 그래디언트의 연결을 끊을 수 있는 몇 가지 덜 명확한 방법이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHDzDOiQ8xmw"
   },
   "source": [
    "### 1. 변수를 텐서로 대체했습니다.\n",
    "\n",
    "[ \"테이프의 감시 대상 제어\"{/ a0} 섹션에서 테이프가 자동으로 `tf.Variable`을 감시하지만 `tf.Tensor`는 감시하지 않는 것을 살펴보았습니다.]()\n",
    "\n",
    "한 가지 일반적인 오류는 `Variable.assign`를 사용하여 `tf.Variable`를 업데이트하는 대신 실수로 `tf.Variable`을 `tf.Tensor`로 대체하는 것입니다. 예를 들면, 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.834863Z",
     "iopub.status.busy": "2021-08-14T00:35:08.833710Z",
     "iopub.status.idle": "2021-08-14T00:35:08.838660Z",
     "shell.execute_reply": "2021-08-14T00:35:08.839101Z"
    },
    "id": "QPKY4Tn9zX7_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "  with tf.GradientTape() as tape:\n",
    "    y = x+1\n",
    "\n",
    "  print(type(x).__name__, \":\", tape.gradient(y, x))\n",
    "#   x = x + 1   # This should be `x.assign_add(1)`\n",
    "  x.assign_add(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gwZKxgA97an"
   },
   "source": [
    "### 2. TensorFlow 외부에서 계산했습니다.\n",
    "\n",
    "계산에서 TensorFlow를 종료하면 테이프가 그래디언트 경로를 기록할 수 없습니다. 예를 들면, 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.845105Z",
     "iopub.status.busy": "2021-08-14T00:35:08.844080Z",
     "iopub.status.idle": "2021-08-14T00:35:08.848529Z",
     "shell.execute_reply": "2021-08-14T00:35:08.847968Z"
    },
    "id": "jmoLCDJb_yw1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  x2 = x**2\n",
    "\n",
    "  # This step is calculated with NumPy\n",
    "#   y = np.mean(x2, axis=0)   # [5,10]\n",
    "#   print(y)\n",
    "  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
    "  # using `tf.convert_to_tensor`.\n",
    "  y = tf.reduce_mean(x2, axis=0)\n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3YVfP3R-tp7"
   },
   "source": [
    "### 3. 정수 또는 문자열을 통해 그래디언트를 구했습니다.\n",
    "\n",
    "정수와 문자열은 구별할 수 없습니다. 계산 경로에서 이러한 데이터 유형을 사용하면 그래디언트는 없습니다.\n",
    "\n",
    "아무도 문자열을 미분할 것으로 기대하지는 않지만, `dtype`을 지정하지 않으면 실수로 `int` 상수 또는 변수를 만들기 쉽습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.853818Z",
     "iopub.status.busy": "2021-08-14T00:35:08.852934Z",
     "iopub.status.idle": "2021-08-14T00:35:08.857672Z",
     "shell.execute_reply": "2021-08-14T00:35:08.858052Z"
    },
    "id": "9jlHXHqfASU3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(20.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(10.)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "\n",
    "print(g.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsdP_mTHX9L1"
   },
   "source": [
    "TensorFlow는 유형 간에 자동으로 전송되지 않으므로 실제로 그래디언트가 누락되는 대신 유형 오류가 발생합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyAZ7C8qCEs6"
   },
   "source": [
    "### 5. 상태 저장 개체를 통해 그래디언트를 구했습니다.\n",
    "\n",
    "상태가 그래디언트를 중지합니다. 상태 저장 객체에서 읽을 때 테이프는 현재 상태만 볼 수 있으며 현재 상태에 이르게 된 기록은 볼 수 없습니다.\n",
    "\n",
    "`tf.Tensor`는 변경할 수 없습니다. 텐서가 작성된 후에는 변경할 수 없습니다. *값*은 있지만 *상태는* 없습니다. 지금까지 논의된 모든 연산은 상태 비저장입니다. `tf.matmul`의 출력은 입력에만 의존합니다.\n",
    "\n",
    "`tf.Variable`은 내부 상태와 값을 갖습니다. 변수를 사용하면 상태를 읽습니다. 변수와 관련하여 그래디언트를 계산하는 것이 일반적이지만, 변수의 상태는 그래디언트 계산이 더 멀리 돌아가지 않도록 차단합니다. 예를 들면, 다음과 같습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.862166Z",
     "iopub.status.busy": "2021-08-14T00:35:08.860396Z",
     "iopub.status.idle": "2021-08-14T00:35:08.865877Z",
     "shell.execute_reply": "2021-08-14T00:35:08.866235Z"
    },
    "id": "C1tLeeRFE479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x0 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # Update x1 = x1 + x0.\n",
    "  x1.assign_add(x0)\n",
    "  # The tape starts recording from x1.\n",
    "  y = x1**2   # y = (x1 + x0)**2  =>  2*x1+2*x0\n",
    "\n",
    "# This doesn't work.\n",
    "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKA92-dqF2r-"
   },
   "source": [
    "마찬가지로, `tf.data.Dataset` 반복기(iterator)와 `tf.queue`는 상태 저장이며 이들을 통과하는 텐서의 모든 그래디언트를 중지합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHvcDGIbOj2I"
   },
   "source": [
    "## 그래디언트가 등록되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoc-A6AxVqry"
   },
   "source": [
    "일부 `tf.Operation`는 **미분 불가능한 것으로 등록되어** `None`을 반환합니다. 다른 연산에는 **그래디언트가 등록되지 않았습니다**.\n",
    "\n",
    "`tf.raw_ops` 페이지에는 그래디언트가 등록된 저수준 op가 표시됩니다.\n",
    "\n",
    "그래디언트가 등록되지 않은 float op를 통해 그래디언트를 얻고자 시도하면 테이프가 자동으로 `None`을 반환하는 대신 오류가 발생합니다. 이렇게 하면 무언가 잘못되었다는 것을 알 수 있습니다.\n",
    "\n",
    "예를 들어, `tf.image.adjust_contrast` 함수는 그래디언트를 가질 수 있지만 그래디언트는 구현되지 않은 `raw_ops.AdjustContrastv2`를 래핑합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.871525Z",
     "iopub.status.busy": "2021-08-14T00:35:08.870920Z",
     "iopub.status.idle": "2021-08-14T00:35:08.875724Z",
     "shell.execute_reply": "2021-08-14T00:35:08.875264Z"
    },
    "id": "HSb20FXc_V0U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LookupError: gradient registry has no entry for: AdjustContrastv2\n"
     ]
    }
   ],
   "source": [
    "image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
    "delta = tf.Variable(0.1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  new_image = tf.image.adjust_contrast(image, delta)\n",
    "\n",
    "try:\n",
    "  print(tape.gradient(new_image, [image, delta]))\n",
    "  assert False   # This should not happen.\n",
    "except LookupError as e:\n",
    "  print(f'{type(e).__name__}: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDoutjzATiEm"
   },
   "source": [
    "이 op를 통해 미분해야 하는 경우, 그래디언트를 구현하고 등록하거나(`tf.RegisterGradient` 사용) 다른 ops를 사용하여 함수를 다시 구현해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCTwc_dQXp2W"
   },
   "source": [
    "## None 대신 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYDrVogA89eA"
   },
   "source": [
    "연결되지 않은 그래디언트의 경우 `None` 대신 0을 가져오는 것이 편리한 경우가 있습니다. 연결되지 않은 그래디언트가 있을 때 `unconnected_gradients` 인수를 사용하여 반환할 항목을 결정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-14T00:35:08.880475Z",
     "iopub.status.busy": "2021-08-14T00:35:08.879855Z",
     "iopub.status.idle": "2021-08-14T00:35:08.884181Z",
     "shell.execute_reply": "2021-08-14T00:35:08.883636Z"
    },
    "id": "U6zxk1sf9Ixx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2., 2.])\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = y**2\n",
    "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "name": "autodiff.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
